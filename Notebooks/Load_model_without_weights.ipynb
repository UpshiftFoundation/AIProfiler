{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7d19786c6b24f869f606a44c3977d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e597290bb22c4ca28d6e2705d67873da",
              "IPY_MODEL_a7781311293b44c582ecfa3eed84b138",
              "IPY_MODEL_8421a990c879401e98dc9a282f0fb450"
            ],
            "layout": "IPY_MODEL_8daf564769f54e558868e1d6def7ae62"
          }
        },
        "e597290bb22c4ca28d6e2705d67873da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dd3df2deb0043f7aac68852a80d1ac2",
            "placeholder": "​",
            "style": "IPY_MODEL_e41586371b8040f8959aa24984910d96",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "a7781311293b44c582ecfa3eed84b138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff60ff9656246acb05f4614fb2e8140",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_004f6a1abad84907bfa11ae1f09005de",
            "value": 548105171
          }
        },
        "8421a990c879401e98dc9a282f0fb450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faffe06613e843c396f7fc0e705cfe28",
            "placeholder": "​",
            "style": "IPY_MODEL_7e8b290ce731473dad7d219828d1a852",
            "value": " 548M/548M [00:03&lt;00:00, 142MB/s]"
          }
        },
        "8daf564769f54e558868e1d6def7ae62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dd3df2deb0043f7aac68852a80d1ac2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e41586371b8040f8959aa24984910d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aff60ff9656246acb05f4614fb2e8140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "004f6a1abad84907bfa11ae1f09005de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "faffe06613e843c396f7fc0e705cfe28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e8b290ce731473dad7d219828d1a852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "422d91a0bf244c3aa8433b6c1d10a174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73b8d828ba544fb2ab8701455da8cf40",
              "IPY_MODEL_748a9e109eb0478fa8d3c6cf671ab2ee",
              "IPY_MODEL_a65d326ace3642a7bfaf157423e3cf03"
            ],
            "layout": "IPY_MODEL_fc2a929e7b8f4fe19c317ed31850dcc3"
          }
        },
        "73b8d828ba544fb2ab8701455da8cf40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77f9b1cad4c64ae6bd2a032b51899966",
            "placeholder": "​",
            "style": "IPY_MODEL_475d714b4035471483ff7267aacc9da6",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "748a9e109eb0478fa8d3c6cf671ab2ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aea1d234dad4842bc222df94b4c985d",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdb4001a4374465188978357637c103c",
            "value": 548118077
          }
        },
        "a65d326ace3642a7bfaf157423e3cf03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff43bca06d084edbb1559af3c0c93c60",
            "placeholder": "​",
            "style": "IPY_MODEL_6ccfbe669fd64ebc8ac26cff6d7693e3",
            "value": " 548M/548M [00:02&lt;00:00, 217MB/s]"
          }
        },
        "fc2a929e7b8f4fe19c317ed31850dcc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f9b1cad4c64ae6bd2a032b51899966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475d714b4035471483ff7267aacc9da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aea1d234dad4842bc222df94b4c985d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdb4001a4374465188978357637c103c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff43bca06d084edbb1559af3c0c93c60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ccfbe669fd64ebc8ac26cff6d7693e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b82c1ac52f64d31a60a33ef85c34994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58c62861f6634204b82c11e14a62cd7b",
              "IPY_MODEL_d18732a49bec4a18bd0db58f948cb7f0",
              "IPY_MODEL_bf01be65a95740b29995371d30e10b4e"
            ],
            "layout": "IPY_MODEL_56ef6220e5a543c5a41b045a13afabf0"
          }
        },
        "58c62861f6634204b82c11e14a62cd7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a84c60444632481d98b8dfbb8d640c89",
            "placeholder": "​",
            "style": "IPY_MODEL_5f769de6ee844593a66aea52ccf3d422",
            "value": ""
          }
        },
        "d18732a49bec4a18bd0db58f948cb7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dedfb782b19843f2b48dc8ea7d4e5c88",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11cd9fdecf1649b28930777b503a789a",
            "value": 0
          }
        },
        "bf01be65a95740b29995371d30e10b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b481d5824424ef6ae2cc2afb5f47c1e",
            "placeholder": "​",
            "style": "IPY_MODEL_7785884590e645dd9b94f956406ba6e0",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "56ef6220e5a543c5a41b045a13afabf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a84c60444632481d98b8dfbb8d640c89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f769de6ee844593a66aea52ccf3d422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dedfb782b19843f2b48dc8ea7d4e5c88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "11cd9fdecf1649b28930777b503a789a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b481d5824424ef6ae2cc2afb5f47c1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7785884590e645dd9b94f956406ba6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6582a57b7724a2d8ed49882e604d2b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_becae544e800441eac6de5f6b27ece51",
              "IPY_MODEL_d1c12081b8d449898c15685054f6de34",
              "IPY_MODEL_20c24346fb974c8c891fee9e15343bdd"
            ],
            "layout": "IPY_MODEL_a9dfcc5a23fa45ee85bbf12c821e1471"
          }
        },
        "becae544e800441eac6de5f6b27ece51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bda1dae10714de3bcdeff2e4def383b",
            "placeholder": "​",
            "style": "IPY_MODEL_a6872e723fcc4e6ba96696dfa9e16954",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "d1c12081b8d449898c15685054f6de34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82cca18689984eceb6f5c7f694411db0",
            "max": 651,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61ac5cf95013493ba91133e411b3958f",
            "value": 651
          }
        },
        "20c24346fb974c8c891fee9e15343bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_630d32402aa04cb49c1cdb38605f5af6",
            "placeholder": "​",
            "style": "IPY_MODEL_9c33879bc28d4cecb7b28ae10184baab",
            "value": " 651/651 [00:00&lt;00:00, 23.2kB/s]"
          }
        },
        "a9dfcc5a23fa45ee85bbf12c821e1471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bda1dae10714de3bcdeff2e4def383b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6872e723fcc4e6ba96696dfa9e16954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82cca18689984eceb6f5c7f694411db0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61ac5cf95013493ba91133e411b3958f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "630d32402aa04cb49c1cdb38605f5af6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c33879bc28d4cecb7b28ae10184baab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8ff2afd81fe4207bf62eadd256dca0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea14219ed8d24205b0729bdf3b04fd0c",
              "IPY_MODEL_4aeb13c31ca54d22a706dc2eb95f05a1",
              "IPY_MODEL_4d161d390b6147cb9906eecc53bfbbdc"
            ],
            "layout": "IPY_MODEL_da1349df4be84311b075f7656a7f1ce9"
          }
        },
        "ea14219ed8d24205b0729bdf3b04fd0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd00c594c4514ec7bd38bcb987c04063",
            "placeholder": "​",
            "style": "IPY_MODEL_8525eaf0029c45689e527b1ee65221b6",
            "value": "Downloading (…)model.bin.index.json: 100%"
          }
        },
        "4aeb13c31ca54d22a706dc2eb95f05a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a8ad8ffc0264164af7f23c3b7328642",
            "max": 41937,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e41d3160516a4111886fb96532bb288f",
            "value": 41937
          }
        },
        "4d161d390b6147cb9906eecc53bfbbdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45927736f8f5402881f33e157731a27a",
            "placeholder": "​",
            "style": "IPY_MODEL_1901a07c48c2454e8461644ec0e8984a",
            "value": " 41.9k/41.9k [00:00&lt;00:00, 669kB/s]"
          }
        },
        "da1349df4be84311b075f7656a7f1ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd00c594c4514ec7bd38bcb987c04063": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8525eaf0029c45689e527b1ee65221b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a8ad8ffc0264164af7f23c3b7328642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e41d3160516a4111886fb96532bb288f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45927736f8f5402881f33e157731a27a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1901a07c48c2454e8461644ec0e8984a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a60a8a2d20a4b6fb0f3e791d55dfaa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed1cf706337f4a44bcd4e0478d00a949",
              "IPY_MODEL_d555cdfcf77f4b4bbd6241960eacfaa7",
              "IPY_MODEL_45a88bca1a654dc2af56bcfe50659d0e"
            ],
            "layout": "IPY_MODEL_b2de3db8401a4b5783dd6c875d10464b"
          }
        },
        "ed1cf706337f4a44bcd4e0478d00a949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb9c3f5fcdc944a0beecc2ec45e9a122",
            "placeholder": "​",
            "style": "IPY_MODEL_94cfa57259a449e5806d3133d3442bc4",
            "value": "Downloading shards: 100%"
          }
        },
        "d555cdfcf77f4b4bbd6241960eacfaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c051711fbbbe4dfeb23515d89ec6a41f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7df5a6a2f5f14d7e8c340d50fd719825",
            "value": 2
          }
        },
        "45a88bca1a654dc2af56bcfe50659d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00b4617723de4b20bee300b3328f3515",
            "placeholder": "​",
            "style": "IPY_MODEL_42954974cb1a4e48a6ba7e73462d49c5",
            "value": " 2/2 [01:58&lt;00:00, 54.56s/it]"
          }
        },
        "b2de3db8401a4b5783dd6c875d10464b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb9c3f5fcdc944a0beecc2ec45e9a122": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94cfa57259a449e5806d3133d3442bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c051711fbbbe4dfeb23515d89ec6a41f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df5a6a2f5f14d7e8c340d50fd719825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00b4617723de4b20bee300b3328f3515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42954974cb1a4e48a6ba7e73462d49c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77e836f4047d491aa344114ec29340db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bef02b41380e41bc81c6801520c9fb55",
              "IPY_MODEL_ba415cc3c47d40cbb60ba371c21ec2bb",
              "IPY_MODEL_40f0ebd6791045e9a843db5f577dc60f"
            ],
            "layout": "IPY_MODEL_3945b1af99824ef3904e095d259768a1"
          }
        },
        "bef02b41380e41bc81c6801520c9fb55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74bbb129890f4e1c9b79fc23a6eeb480",
            "placeholder": "​",
            "style": "IPY_MODEL_16e13a02ac9b4bb2bad24721a4adfcdd",
            "value": "Downloading (…)l-00001-of-00002.bin: 100%"
          }
        },
        "ba415cc3c47d40cbb60ba371c21ec2bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1cac0c108d64e888536794b5eedfaec",
            "max": 9960750957,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4906878581f94edaab157dccc5f14ce2",
            "value": 9960750957
          }
        },
        "40f0ebd6791045e9a843db5f577dc60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a3b3da8bd62418692fd56d86524655e",
            "placeholder": "​",
            "style": "IPY_MODEL_700684d6bd3a42dda3dbbe3a3184ffbe",
            "value": " 9.96G/9.96G [01:25&lt;00:00, 201MB/s]"
          }
        },
        "3945b1af99824ef3904e095d259768a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74bbb129890f4e1c9b79fc23a6eeb480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16e13a02ac9b4bb2bad24721a4adfcdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1cac0c108d64e888536794b5eedfaec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4906878581f94edaab157dccc5f14ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a3b3da8bd62418692fd56d86524655e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "700684d6bd3a42dda3dbbe3a3184ffbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4127d9edc8ba4cb182fc775b47e74125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb04008488174c07a35ec11f941b7cdb",
              "IPY_MODEL_2b332e5ef7994b4abb19c098eebcf577",
              "IPY_MODEL_bcee3b82a55840f48618cf721979aba8"
            ],
            "layout": "IPY_MODEL_d67ab9cea2dc44579276efbbdd843190"
          }
        },
        "fb04008488174c07a35ec11f941b7cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d763f6a3d9c8438180ec3373f8a91996",
            "placeholder": "​",
            "style": "IPY_MODEL_f498acb03d6a4cb2ba3bb18a45e920cd",
            "value": "Downloading (…)l-00002-of-00002.bin: 100%"
          }
        },
        "2b332e5ef7994b4abb19c098eebcf577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed6e5c9be66e4b77828f71f1e4c0f8dd",
            "max": 3356360185,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36575f730daa47569b0c8f17ee2e4edb",
            "value": 3356360185
          }
        },
        "bcee3b82a55840f48618cf721979aba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0eb81c77c994a6cae4f5d21e0b6c594",
            "placeholder": "​",
            "style": "IPY_MODEL_422ea9e756fb4c76bef40a8d7f12856e",
            "value": " 3.36G/3.36G [00:31&lt;00:00, 141MB/s]"
          }
        },
        "d67ab9cea2dc44579276efbbdd843190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d763f6a3d9c8438180ec3373f8a91996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f498acb03d6a4cb2ba3bb18a45e920cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed6e5c9be66e4b77828f71f1e4c0f8dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36575f730daa47569b0c8f17ee2e4edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0eb81c77c994a6cae4f5d21e0b6c594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "422ea9e756fb4c76bef40a8d7f12856e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "835e8d3b9ea649d0967f183e89888349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dcd68e71d444a0d99daecc3e4f37fe8",
              "IPY_MODEL_0b159b68c42b4eedaf71c6142e3a7014",
              "IPY_MODEL_054f8c687c804552a8e36d099020e20c"
            ],
            "layout": "IPY_MODEL_0d3278dc2b3647a5bd1caa1ba67c415a"
          }
        },
        "1dcd68e71d444a0d99daecc3e4f37fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7178692a59234105a06d27bc5fa2ed69",
            "placeholder": "​",
            "style": "IPY_MODEL_87bfec3fe6d44a1989926182ae91689e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0b159b68c42b4eedaf71c6142e3a7014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_758bee854a5b4184a9d7bf38ebc0e580",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd3ee9860c8942c29c56c5aa48043c25",
            "value": 2
          }
        },
        "054f8c687c804552a8e36d099020e20c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_978521f7ff394fbdae2b6bf79071d382",
            "placeholder": "​",
            "style": "IPY_MODEL_0f2d549895b34f8787c0af1142c95714",
            "value": " 2/2 [01:05&lt;00:00, 30.06s/it]"
          }
        },
        "0d3278dc2b3647a5bd1caa1ba67c415a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7178692a59234105a06d27bc5fa2ed69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87bfec3fe6d44a1989926182ae91689e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "758bee854a5b4184a9d7bf38ebc0e580": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd3ee9860c8942c29c56c5aa48043c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "978521f7ff394fbdae2b6bf79071d382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f2d549895b34f8787c0af1142c95714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dd11ac6ed5d4292ae605b7af95e6d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8c8867372044f1f95f701e1a5fd21d3",
              "IPY_MODEL_5a7e8bbc60de4e619a1e0fa8888da17b",
              "IPY_MODEL_beb4693285104b67827789b8a7f43207"
            ],
            "layout": "IPY_MODEL_dbe83ffc375241a7ba65418c2bfeda6c"
          }
        },
        "d8c8867372044f1f95f701e1a5fd21d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cb1c726096049498f7a0bc015fdbe48",
            "placeholder": "​",
            "style": "IPY_MODEL_4e8d4c19d1734de6a8e9141497c669f8",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "5a7e8bbc60de4e619a1e0fa8888da17b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa5424f8fde4c62aecb2eb6cf1a54f8",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a33407163c8245d6b6750f7f2c597e4c",
            "value": 137
          }
        },
        "beb4693285104b67827789b8a7f43207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28af3ea3c7724ce2962d30b9f5265106",
            "placeholder": "​",
            "style": "IPY_MODEL_7f46907d07f740d4b18b6b261acc3937",
            "value": " 137/137 [00:00&lt;00:00, 7.84kB/s]"
          }
        },
        "dbe83ffc375241a7ba65418c2bfeda6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cb1c726096049498f7a0bc015fdbe48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e8d4c19d1734de6a8e9141497c669f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8aa5424f8fde4c62aecb2eb6cf1a54f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a33407163c8245d6b6750f7f2c597e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28af3ea3c7724ce2962d30b9f5265106": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f46907d07f740d4b18b6b261acc3937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a42b643e78646c4977affe2260eee32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60eff463678f4948a23ebcdfaf412963",
              "IPY_MODEL_32086e6bc0204dd2a6c3b607f2074490",
              "IPY_MODEL_8ac6462663bc445e8e440d255c393d16"
            ],
            "layout": "IPY_MODEL_ae8b47cebed6443e86071e7bd963d248"
          }
        },
        "60eff463678f4948a23ebcdfaf412963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da7be676318442d592bacc00ec35e790",
            "placeholder": "​",
            "style": "IPY_MODEL_3d3f32f82dac45ac86dcb142f0fac61d",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "32086e6bc0204dd2a6c3b607f2074490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7ccbaa57c5c44a698d0574d36923837",
            "max": 685,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed446c74f5a2403082ce7faad2e1ee33",
            "value": 685
          }
        },
        "8ac6462663bc445e8e440d255c393d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50bdc6257eb644aaa42406353398251d",
            "placeholder": "​",
            "style": "IPY_MODEL_d3e56c8dfe3d491b86eb8b821c457cb7",
            "value": " 685/685 [00:00&lt;00:00, 38.3kB/s]"
          }
        },
        "ae8b47cebed6443e86071e7bd963d248": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da7be676318442d592bacc00ec35e790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d3f32f82dac45ac86dcb142f0fac61d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7ccbaa57c5c44a698d0574d36923837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed446c74f5a2403082ce7faad2e1ee33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50bdc6257eb644aaa42406353398251d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3e56c8dfe3d491b86eb8b821c457cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8317f658e45f4b6e861da6f934928767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e951aea7b36d492b9099482501284961",
              "IPY_MODEL_ab17420b744840aaa643af4c2a400723",
              "IPY_MODEL_ab5dfaed495a407fa80d814b90d90bb8"
            ],
            "layout": "IPY_MODEL_92ec9f66680a4d8fab65be063c39e413"
          }
        },
        "e951aea7b36d492b9099482501284961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d41c5340e9e4d1fbd0a70587809d01f",
            "placeholder": "​",
            "style": "IPY_MODEL_cbf7c76de05f4a4e942ba835f135cc40",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "ab17420b744840aaa643af4c2a400723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b294edc01044489b05c6af2746fb804",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42d2904184bd4bf59e31ab5661560816",
            "value": 898822
          }
        },
        "ab5dfaed495a407fa80d814b90d90bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2104a5315b6c4825833b0401b5107de0",
            "placeholder": "​",
            "style": "IPY_MODEL_90d48263d5c540be8db9c3550304ad7c",
            "value": " 899k/899k [00:00&lt;00:00, 2.89MB/s]"
          }
        },
        "92ec9f66680a4d8fab65be063c39e413": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d41c5340e9e4d1fbd0a70587809d01f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbf7c76de05f4a4e942ba835f135cc40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b294edc01044489b05c6af2746fb804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42d2904184bd4bf59e31ab5661560816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2104a5315b6c4825833b0401b5107de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90d48263d5c540be8db9c3550304ad7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ba63e6adbbf41489118b816146908b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90b150b685c34722a58b91d603656347",
              "IPY_MODEL_107f089f4a06426f9e33059083cbd72c",
              "IPY_MODEL_c96a95dde2f14dc8a35e22ab6ca80e5d"
            ],
            "layout": "IPY_MODEL_98c9418007c347008af8dede777ae4d5"
          }
        },
        "90b150b685c34722a58b91d603656347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce365ca8acc44be9a793666f0d678c47",
            "placeholder": "​",
            "style": "IPY_MODEL_57cd473c7f2c4d33915d7bc913bbf0f5",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "107f089f4a06426f9e33059083cbd72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23fcaaae605d4cd5a47b1f236ce67989",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edd3723039c54c088f62689272017844",
            "value": 456318
          }
        },
        "c96a95dde2f14dc8a35e22ab6ca80e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bce244d0bf44c3e89656e017ad21390",
            "placeholder": "​",
            "style": "IPY_MODEL_4dd2925ed83b4ca2909f602510d39b75",
            "value": " 456k/456k [00:00&lt;00:00, 1.74MB/s]"
          }
        },
        "98c9418007c347008af8dede777ae4d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce365ca8acc44be9a793666f0d678c47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57cd473c7f2c4d33915d7bc913bbf0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23fcaaae605d4cd5a47b1f236ce67989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd3723039c54c088f62689272017844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2bce244d0bf44c3e89656e017ad21390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dd2925ed83b4ca2909f602510d39b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91a3b12eb14c48e7a6f8c45a2727ea3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d45e8e0e588c45ecaf28f814b3ed5fd6",
              "IPY_MODEL_aa7e038ec848466698751ffdc57f9a1b",
              "IPY_MODEL_da10e521a579448891808faf86e8e943"
            ],
            "layout": "IPY_MODEL_547388870c9e48779cc6a18c37b4c6fa"
          }
        },
        "d45e8e0e588c45ecaf28f814b3ed5fd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5bfba19e63a46828f63e49a5888b790",
            "placeholder": "​",
            "style": "IPY_MODEL_075cbe3e382b4a0ba20d161b2b28d785",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "aa7e038ec848466698751ffdc57f9a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d02140d1701d4894943334b5c4ead0a6",
            "max": 441,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bbcf5d0eaab490b84d2b9a72ddf9ed1",
            "value": 441
          }
        },
        "da10e521a579448891808faf86e8e943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d872cc4c5171418982bbe36fe8c389d2",
            "placeholder": "​",
            "style": "IPY_MODEL_e1a61ef5f77844e998701953501e7e5a",
            "value": " 441/441 [00:00&lt;00:00, 23.9kB/s]"
          }
        },
        "547388870c9e48779cc6a18c37b4c6fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5bfba19e63a46828f63e49a5888b790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "075cbe3e382b4a0ba20d161b2b28d785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d02140d1701d4894943334b5c4ead0a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bbcf5d0eaab490b84d2b9a72ddf9ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d872cc4c5171418982bbe36fe8c389d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a61ef5f77844e998701953501e7e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "447a2e797a3f4422adf6f536ca56bc5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f897fac9a189478eb7f56992b99aca5f",
              "IPY_MODEL_81792d2dd8e8481f9200c6541eb21f33",
              "IPY_MODEL_b8dc54b793644821be860c21af9636b1"
            ],
            "layout": "IPY_MODEL_bacadd32d2354cde9f1580de6b083310"
          }
        },
        "f897fac9a189478eb7f56992b99aca5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_876b4712469e4283b08dc210ee600d2a",
            "placeholder": "​",
            "style": "IPY_MODEL_b2470fcb40cf4f14a965d22e11a88870",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "81792d2dd8e8481f9200c6541eb21f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76566b41eb1c46bfa998ea98cf5bd3f1",
            "max": 651,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73eff778d00c4580ac64285d2c7b4e83",
            "value": 651
          }
        },
        "b8dc54b793644821be860c21af9636b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6feee78b9a6f4a489282e0e170e7d51f",
            "placeholder": "​",
            "style": "IPY_MODEL_9541c8e2b2084e64aaae5e4cf30e161f",
            "value": " 651/651 [00:00&lt;00:00, 18.0kB/s]"
          }
        },
        "bacadd32d2354cde9f1580de6b083310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "876b4712469e4283b08dc210ee600d2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2470fcb40cf4f14a965d22e11a88870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76566b41eb1c46bfa998ea98cf5bd3f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73eff778d00c4580ac64285d2c7b4e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6feee78b9a6f4a489282e0e170e7d51f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9541c8e2b2084e64aaae5e4cf30e161f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e342ab1c4b2d4c5399d45da305cf19dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3419cef609194fca86834b08ec555837",
              "IPY_MODEL_df8d94c8e3904d19bc3bd081f6018b5c",
              "IPY_MODEL_fda2b81df2094eeab7a916c4617655a0"
            ],
            "layout": "IPY_MODEL_03f616b01c4c4405a6e1f3fe6d93a918"
          }
        },
        "3419cef609194fca86834b08ec555837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8a909e43db945f4b9d50a60bc2ca4c8",
            "placeholder": "​",
            "style": "IPY_MODEL_cc7c6b8c75aa432d954a45a48efa754f",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "df8d94c8e3904d19bc3bd081f6018b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_353c8a995d6847c7a575ed967acf78e5",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61df1e0ad3e54adca7f72dd349c78e72",
            "value": 800
          }
        },
        "fda2b81df2094eeab7a916c4617655a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97cea7d5720a4eb08b2343ca42983581",
            "placeholder": "​",
            "style": "IPY_MODEL_cdb699f8d1ba4df4b731c3422c203057",
            "value": " 800/800 [00:00&lt;00:00, 26.2kB/s]"
          }
        },
        "03f616b01c4c4405a6e1f3fe6d93a918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8a909e43db945f4b9d50a60bc2ca4c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7c6b8c75aa432d954a45a48efa754f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "353c8a995d6847c7a575ed967acf78e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61df1e0ad3e54adca7f72dd349c78e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97cea7d5720a4eb08b2343ca42983581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdb699f8d1ba4df4b731c3422c203057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mA-pWspDzNe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install torch\n",
        "%pip install transformers\n",
        "%pip install accelerate\n",
        "%pip install safetensors\n",
        "%pip install pytorch_memlab\n",
        "%pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOOK**"
      ],
      "metadata": {
        "id": "U3JEkacLccKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory Profiler"
      ],
      "metadata": {
        "id": "Nw9J-I_1v31I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBq-fGwku4cO",
        "outputId": "3832dad1-ced4-4230-b39e-fea7459586d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.9/dist-packages (0.61.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from memory_profiler) (5.9.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***-------------------------------[1]-------------------------------***"
      ],
      "metadata": {
        "id": "qrWz5ykhdfj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import infer_auto_device_map, init_empty_weights\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "def func():\n",
        "  #config = AutoConfig.from_pretrained(\"decapoda-research/llama-65b-hf\", low_cpu_mem_usage=True)\n",
        "  config = AutoConfig.from_pretrained(\"pszemraj/flan-t5-large-grammar-synthesis\")\n",
        "  with init_empty_weights():\n",
        "    model = AutoModelForSeq2SeqLM.from_config(config) \n",
        "  device_map = infer_auto_device_map(model)\n"
      ],
      "metadata": {
        "id": "0ZGxb1v-RCFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cProfile\n",
        "import re\n",
        "cProfile.run('model = AutoModelForSeq2SeqLM.from_config(config)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9Q0oHDie-Ze",
        "outputId": "80246923-715e-4e53-86ae-28a35ed41df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         135910 function calls (113779 primitive calls) in 18.563 seconds\n",
            "\n",
            "   Ordered by: standard name\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.000    0.000   18.563   18.563 <string>:1(<module>)\n",
            "        1    0.000    0.000    0.000    0.000 __init__.py:1436(info)\n",
            "        1    0.000    0.000    0.000    0.000 __init__.py:1689(isEnabledFor)\n",
            "        1    0.000    0.000    0.000    0.000 __init__.py:183(dumps)\n",
            "       38    0.000    0.000    0.000    0.000 __init__.py:97(_xla_gc_callback)\n",
            "      436    0.000    0.000    0.000    0.000 _jit_internal.py:1082(is_scripting)\n",
            "        2    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
            "       48    0.000    0.000    0.002    0.000 activations.py:167(__getitem__)\n",
            "       48    0.000    0.000    0.001    0.000 activations.py:67(__init__)\n",
            "        1    0.000    0.000    0.000    0.000 auto_factory.py:359(_get_model_class)\n",
            "        1    0.000    0.000   18.563   18.563 auto_factory.py:390(from_config)\n",
            "       22    0.000    0.000    0.000    0.000 auto_factory.py:556(getattribute_from_module)\n",
            "        1    0.000    0.000    0.000    0.000 auto_factory.py:596(__getitem__)\n",
            "       22    0.000    0.000    0.000    0.000 auto_factory.py:612(_load_attr_from_module)\n",
            "        1    0.000    0.000    0.000    0.000 auto_factory.py:618(keys)\n",
            "        1    0.000    0.000    0.000    0.000 auto_factory.py:619(<listcomp>)\n",
            "       22    0.000    0.000    0.000    0.000 configuration_auto.py:592(model_type_to_module_name)\n",
            "        2    0.000    0.000    0.000    0.000 configuration_utils.py:222(__init__)\n",
            "        6    0.000    0.000    0.000    0.000 configuration_utils.py:252(__setattr__)\n",
            "2887/2884    0.008    0.000    0.008    0.000 configuration_utils.py:257(__getattribute__)\n",
            "        1    0.000    0.000    0.001    0.001 configuration_utils.py:312(__repr__)\n",
            "        2    0.000    0.000    0.000    0.000 configuration_utils.py:315(validate)\n",
            "        3    0.000    0.000    0.000    0.000 configuration_utils.py:393(name_or_path)\n",
            "        1    0.000    0.000    0.001    0.001 configuration_utils.py:547(from_dict)\n",
            "      5/3    0.000    0.000    0.000    0.000 configuration_utils.py:581(dict_torch_dtype_to_str)\n",
            "        1    0.000    0.000    0.001    0.001 configuration_utils.py:593(to_diff_dict)\n",
            "        2    0.000    0.000    0.000    0.000 configuration_utils.py:616(to_dict)\n",
            "        1    0.000    0.000    0.001    0.001 configuration_utils.py:633(to_json_string)\n",
            "        1    0.000    0.000    0.001    0.001 configuration_utils.py:665(from_model_config)\n",
            "        1    0.000    0.000    0.000    0.000 configuration_utils.py:694(update)\n",
            "        1    0.000    0.000    0.000    0.000 configuration_utils.py:713(<dictcomp>)\n",
            "        1    0.000    0.000    0.001    0.001 configuration_utils.py:786(to_dict)\n",
            "      4/1    0.000    0.000    0.000    0.000 configuration_utils.py:888(dict_torch_dtype_to_str)\n",
            "       50    0.000    0.000    0.003    0.000 container.py:263(__init__)\n",
            "      122    0.000    0.000    0.000    0.000 container.py:298(__len__)\n",
            "        2    0.000    0.000    0.000    0.000 container.py:306(__iadd__)\n",
            "      120    0.001    0.000    0.006    0.000 container.py:332(append)\n",
            "        2    0.000    0.000    0.000    0.000 container.py:346(extend)\n",
            "    674/5    0.001    0.000    0.002    0.000 copy.py:128(deepcopy)\n",
            "      650    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)\n",
            "        3    0.000    0.000    0.000    0.000 copy.py:200(_deepcopy_list)\n",
            "     16/5    0.000    0.000    0.002    0.000 copy.py:226(_deepcopy_dict)\n",
            "       21    0.000    0.000    0.000    0.000 copy.py:242(_keep_alive)\n",
            "        2    0.000    0.000    0.001    0.000 copy.py:258(_reconstruct)\n",
            "        4    0.000    0.000    0.000    0.000 copy.py:263(<genexpr>)\n",
            "        2    0.000    0.000    0.000    0.000 copyreg.py:94(__newobj__)\n",
            "        1    0.000    0.000    0.000    0.000 deepspeed.py:245(is_deepspeed_zero3_enabled)\n",
            "      170    0.001    0.000    0.007    0.000 dropout.py:13(__init__)\n",
            "        1    0.000    0.000    0.000    0.000 encoder.py:104(__init__)\n",
            "        1    0.000    0.000    0.000    0.000 encoder.py:182(encode)\n",
            "        1    0.000    0.000    0.000    0.000 encoder.py:204(iterencode)\n",
            "        1    0.000    0.000    0.000    0.000 encoder.py:259(_make_iterencode)\n",
            "       40    0.000    0.000    0.000    0.000 encoder.py:333(_iterencode_dict)\n",
            "       40    0.000    0.000    0.000    0.000 encoder.py:413(_iterencode)\n",
            "      436    0.010    0.000    0.010    0.000 grad_mode.py:126(__init__)\n",
            "      436    0.001    0.000    0.003    0.000 grad_mode.py:131(__enter__)\n",
            "      436    0.006    0.000    0.011    0.000 grad_mode.py:135(__exit__)\n",
            "      872    0.004    0.000    0.006    0.000 grad_mode.py:227(__init__)\n",
            "        3    0.000    0.000    0.486    0.162 init.py:140(normal_)\n",
            "        3    0.000    0.000    0.486    0.162 init.py:17(_no_grad_normal_)\n",
            "      433    0.002    0.000    0.004    0.000 init.py:284(_calculate_fan_in_and_fan_out)\n",
            "      433    0.002    0.000    0.007    0.000 init.py:356(_calculate_correct_fan)\n",
            "      433    0.014    0.000    9.950    0.023 init.py:366(kaiming_uniform_)\n",
            "      433    0.006    0.000    0.015    0.000 init.py:67(calculate_gain)\n",
            "      433    0.013    0.000    9.971    0.023 linear.py:103(reset_parameters)\n",
            "      433    0.014    0.000   10.071    0.023 linear.py:90(__init__)\n",
            "        1    0.000    0.000   18.562   18.562 modeling_t5.py:1521(__init__)\n",
            "      122    0.002    0.000    0.022    0.000 modeling_t5.py:239(__init__)\n",
            "       48    0.004    0.000    5.942    0.124 modeling_t5.py:302(__init__)\n",
            "       48    0.001    0.000    5.956    0.124 modeling_t5.py:331(__init__)\n",
            "       72    0.005    0.000    3.639    0.051 modeling_t5.py:349(__init__)\n",
            "       48    0.002    0.000    2.879    0.060 modeling_t5.py:583(__init__)\n",
            "       24    0.001    0.000    0.781    0.033 modeling_t5.py:615(__init__)\n",
            "       48    0.002    0.000    9.627    0.201 modeling_t5.py:651(__init__)\n",
            "     1117    0.032    0.000    7.837    0.007 modeling_t5.py:787(_init_weights)\n",
            "        2    0.000    0.000   16.702    8.351 modeling_t5.py:862(__init__)\n",
            "        2    0.000    0.000    9.628    4.814 modeling_t5.py:869(<listcomp>)\n",
            "        3    0.000    0.000    0.002    0.001 modeling_utils.py:1043(__init__)\n",
            "        3    0.000    0.000    7.912    2.637 modeling_utils.py:1057(post_init)\n",
            "        3    0.000    0.000    0.000    0.000 modeling_utils.py:1065(_backward_compatibility_gradient_checkpointing)\n",
            "        1    0.000    0.000   18.562   18.562 modeling_utils.py:1071(_from_config)\n",
            "        3    0.000    0.000    0.000    0.000 modeling_utils.py:1138(can_generate)\n",
            "     2235    0.004    0.000    7.858    0.004 modeling_utils.py:1208(_initialize_weights)\n",
            "        3    0.001    0.000    0.025    0.008 modeling_utils.py:1217(tie_weights)\n",
            "        3    0.000    0.000    7.912    2.637 modeling_utils.py:1542(init_weights)\n",
            "     7071    0.030    0.000    0.035    0.000 module.py:1256(__getattr__)\n",
            "     4885    0.022    0.000    0.064    0.000 module.py:1272(__setattr__)\n",
            "     1508    0.002    0.000    0.002    0.000 module.py:1273(remove_from)\n",
            "     4467    0.014    0.000    0.022    0.000 module.py:1790(children)\n",
            "     4467    0.007    0.000    0.008    0.000 module.py:1799(named_children)\n",
            "     2236    0.001    0.000    0.013    0.000 module.py:1820(modules)\n",
            "16998/2236    0.011    0.000    0.012    0.000 module.py:1847(named_modules)\n",
            "        3    0.000    0.000    0.000    0.000 module.py:1986(_get_name)\n",
            "        3    0.000    0.000    0.000    0.000 module.py:1989(extra_repr)\n",
            "        3    0.000    0.000    0.000    0.000 module.py:1998(__repr__)\n",
            "     1117    0.020    0.000    0.028    0.000 module.py:309(__init__)\n",
            "      991    0.006    0.000    0.014    0.000 module.py:391(register_parameter)\n",
            "      168    0.001    0.000    0.005    0.000 module.py:433(add_module)\n",
            "   2235/3    0.007    0.000    7.887    2.629 module.py:692(apply)\n",
            "      558    0.012    0.000    0.030    0.000 parameter.py:30(__new__)\n",
            "     4327    0.016    0.000    0.020    0.000 parameter.py:8(__instancecheck__)\n",
            "        3    0.000    0.000    0.487    0.162 sparse.py:122(__init__)\n",
            "        3    0.000    0.000    0.486    0.162 sparse.py:150(reset_parameters)\n",
            "        3    0.000    0.000    0.000    0.000 sparse.py:154(_fill_padding_idx_with_zero)\n",
            "        2    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x94bdc0}\n",
            "        2    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
            "       10    0.000    0.000    0.000    0.000 {built-in method _json.encode_basestring_ascii}\n",
            "      558    0.018    0.000    0.018    0.000 {built-in method _make_subclass}\n",
            "        1    0.000    0.000   18.563   18.563 {built-in method builtins.exec}\n",
            "     2280    0.003    0.000    0.007    0.000 {built-in method builtins.getattr}\n",
            "     3564    0.007    0.000    0.024    0.000 {built-in method builtins.hasattr}\n",
            "      722    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
            "22065/17738    0.009    0.000    0.028    0.000 {built-in method builtins.isinstance}\n",
            "        5    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
            "  244/122    0.000    0.000    0.001    0.000 {built-in method builtins.len}\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
            "       38    0.000    0.000    0.000    0.000 {built-in method jaxlib.xla_extension.collect_garbage}\n",
            "     1732    0.009    0.000    0.009    0.000 {built-in method math.sqrt}\n",
            "      436    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_variadic}\n",
            "     1117    0.007    0.000    0.007    0.000 {built-in method torch._C._log_api_usage_once}\n",
            "      872    0.001    0.000    0.001    0.000 {built-in method torch._C._set_grad_enabled}\n",
            "      436    0.025    0.000    0.025    0.000 {built-in method torch.empty}\n",
            "     1308    0.002    0.000    0.002    0.000 {built-in method torch.is_grad_enabled}\n",
            "      122    0.009    0.000    0.009    0.000 {built-in method torch.ones}\n",
            "       48    0.000    0.000    0.000    0.000 {function ClassInstantier.__getitem__ at 0x7f04137b14c0}\n",
            "     4327    0.002    0.000    0.002    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f04b41f7790}\n",
            "        5    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}\n",
            "     4465    0.001    0.000    0.001    0.000 {method 'add' of 'set' objects}\n",
            "       20    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
            "      866    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
            "      122    0.005    0.000    0.005    0.000 {method 'fill_' of 'torch._C._TensorBase' objects}\n",
            "     4509    0.005    0.000    0.005    0.000 {method 'format' of 'str' objects}\n",
            "    13951    0.003    0.000    0.003    0.000 {method 'get' of 'dict' objects}\n",
            "     4472    0.001    0.000    0.001    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
            "       21    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
            "      175    0.000    0.000    0.000    0.000 {method 'keys' of 'collections.OrderedDict' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
            "      433    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}\n",
            "      439    8.275    0.019    8.275    0.019 {method 'normal_' of 'torch._C._TensorBase' objects}\n",
            "      100    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
            "       22    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
            "      866    0.001    0.000    0.001    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
            "      433    9.890    0.023    9.890    0.023 {method 'uniform_' of 'torch._C._TensorBase' objects}\n",
            "        2    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
            "        9    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpkdi1nCSTcd",
        "outputId": "dcedc8e1-d902-4f38-b551-0cc71c984700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 1024)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 1024)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (17): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (18): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (19): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (20): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (21): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (22): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (23): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 1024)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (17): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (18): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (19): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (20): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (21): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (22): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (23): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): GELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G8qkZzvc-X1",
        "outputId": "30a1ef4e-f8bd-45a0-bfc7-8d2a66d85beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 'disk'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is3eCCuBc-Jy",
        "outputId": "55042fc1-2a83-4a46-cd7d-208b85806a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('transformer.word_embeddings.weight',\n",
              "              tensor(..., device='meta', size=(250880, 14336))),\n",
              "             ('transformer.word_embeddings_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.word_embeddings_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.0.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.0.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.0.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.0.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.0.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.0.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.0.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.0.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.0.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.0.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.0.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.0.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.1.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.1.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.1.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.1.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.1.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.1.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.1.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.1.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.1.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.1.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.1.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.1.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.2.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.2.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.2.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.2.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.2.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.2.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.2.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.2.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.2.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.2.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.2.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.2.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.3.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.3.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.3.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.3.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.3.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.3.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.3.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.3.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.3.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.3.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.3.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.3.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.4.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.4.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.4.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.4.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.4.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.4.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.4.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.4.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.4.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.4.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.4.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.4.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.5.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.5.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.5.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.5.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.5.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.5.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.5.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.5.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.5.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.5.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.5.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.5.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.6.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.6.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.6.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.6.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.6.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.6.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.6.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.6.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.6.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.6.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.6.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.6.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.7.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.7.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.7.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.7.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.7.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.7.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.7.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.7.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.7.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.7.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.7.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.7.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.8.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.8.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.8.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.8.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.8.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.8.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.8.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.8.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.8.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.8.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.8.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.8.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.9.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.9.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.9.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.9.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.9.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.9.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.9.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.9.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.9.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.9.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.9.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.9.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.10.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.10.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.10.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.10.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.10.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.10.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.10.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.10.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.10.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.10.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.10.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.10.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.11.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.11.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.11.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.11.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.11.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.11.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.11.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.11.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.11.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.11.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.11.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.11.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.12.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.12.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.12.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.12.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.12.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.12.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.12.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.12.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.12.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.12.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.12.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.12.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.13.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.13.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.13.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.13.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.13.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.13.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.13.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.13.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.13.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.13.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.13.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.13.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.14.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.14.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.14.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.14.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.14.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.14.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.14.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.14.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.14.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.14.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.14.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.14.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.15.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.15.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.15.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.15.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.15.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.15.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.15.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.15.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.15.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.15.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.15.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.15.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.16.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.16.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.16.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.16.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.16.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.16.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.16.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.16.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.16.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.16.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.16.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.16.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.17.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.17.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.17.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.17.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.17.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.17.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.17.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.17.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.17.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.17.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.17.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.17.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.18.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.18.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.18.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.18.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.18.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.18.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.18.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.18.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.18.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.18.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.18.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.18.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.19.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.19.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.19.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.19.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.19.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.19.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.19.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.19.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.19.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.19.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.19.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.19.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.20.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.20.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.20.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.20.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.20.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.20.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.20.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.20.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.20.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.20.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.20.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.20.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.21.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.21.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.21.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.21.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.21.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.21.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.21.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.21.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.21.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.21.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.21.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.21.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.22.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.22.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.22.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.22.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.22.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.22.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.22.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.22.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.22.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.22.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.22.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.22.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.23.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.23.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.23.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.23.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.23.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.23.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.23.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.23.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.23.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.23.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.23.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.23.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.24.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.24.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.24.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.24.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.24.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.24.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.24.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.24.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.24.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.24.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.24.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.24.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.25.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.25.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.25.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.25.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.25.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.25.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.25.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.25.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.25.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.25.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.25.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.25.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.26.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.26.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.26.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.26.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.26.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.26.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.26.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.26.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.26.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.26.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.26.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.26.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.27.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.27.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.27.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.27.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.27.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.27.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.27.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.27.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.27.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.27.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.27.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.27.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.28.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.28.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.28.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.28.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.28.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.28.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.28.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.28.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.28.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.28.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.28.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.28.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.29.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.29.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.29.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.29.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.29.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.29.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.29.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.29.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.29.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.29.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.29.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.29.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.30.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.30.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.30.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.30.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.30.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.30.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.30.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.30.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.30.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.30.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.30.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.30.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.31.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.31.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.31.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.31.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.31.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.31.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.31.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.31.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.31.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.31.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.31.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.31.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.32.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.32.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.32.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.32.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.32.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.32.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.32.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.32.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.32.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.32.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.32.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.32.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.33.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.33.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.33.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.33.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.33.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.33.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.33.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.33.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.33.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.33.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.33.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.33.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.34.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.34.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.34.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.34.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.34.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.34.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.34.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.34.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.34.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.34.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.34.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.34.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.35.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.35.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.35.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.35.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.35.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.35.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.35.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.35.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.35.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.35.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.35.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.35.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.36.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.36.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.36.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.36.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.36.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.36.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.36.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.36.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.36.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.36.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.36.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.36.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.37.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.37.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.37.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.37.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.37.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.37.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.37.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.37.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.37.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.37.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.37.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.37.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.38.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.38.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.38.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.38.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.38.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.38.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.38.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.38.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.38.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.38.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.38.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.38.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.39.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.39.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.39.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.39.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.39.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.39.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.39.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.39.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.39.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.39.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.39.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.39.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.40.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.40.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.40.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.40.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.40.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.40.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.40.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.40.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.40.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.40.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.40.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.40.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.41.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.41.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.41.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.41.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.41.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.41.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.41.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.41.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.41.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.41.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.41.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.41.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.42.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.42.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.42.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.42.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.42.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.42.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.42.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.42.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.42.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.42.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.42.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.42.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.43.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.43.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.43.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.43.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.43.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.43.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.43.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.43.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.43.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.43.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.43.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.43.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.44.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.44.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.44.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.44.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.44.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.44.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.44.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.44.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.44.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.44.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.44.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.44.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.45.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.45.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.45.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.45.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.45.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.45.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.45.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.45.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.45.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.45.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.45.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.45.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.46.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.46.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.46.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.46.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.46.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.46.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.46.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.46.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.46.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.46.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.46.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.46.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.47.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.47.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.47.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.47.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.47.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.47.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.47.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.47.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.47.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.47.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.47.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.47.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.48.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.48.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.48.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.48.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.48.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.48.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.48.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.48.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.48.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.48.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.48.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.48.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.49.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.49.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.49.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.49.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.49.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.49.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.49.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.49.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.49.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.49.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.49.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.49.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.50.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.50.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.50.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.50.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.50.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.50.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.50.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.50.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.50.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.50.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.50.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.50.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.51.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.51.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.51.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.51.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.51.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.51.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.51.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.51.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.51.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.51.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.51.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.51.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.52.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.52.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.52.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.52.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.52.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.52.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.52.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.52.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.52.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.52.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.52.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.52.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.53.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.53.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.53.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.53.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.53.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.53.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.53.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.53.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.53.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.53.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.53.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.53.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.54.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.54.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.54.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.54.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.54.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.54.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.54.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.54.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.54.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.54.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.54.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.54.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.55.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.55.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.55.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.55.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.55.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.55.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.55.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.55.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.55.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.55.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.55.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.55.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.56.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.56.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.56.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.56.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.56.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.56.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.56.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.56.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.56.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.56.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.56.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.56.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.57.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.57.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.57.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.57.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.57.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.57.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.57.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.57.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.57.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.57.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.57.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.57.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.58.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.58.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.58.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.58.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.58.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.58.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.58.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.58.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.58.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.58.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.58.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.58.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.59.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.59.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.59.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.59.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.59.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.59.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.59.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.59.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.59.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.59.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.59.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.59.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.60.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.60.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.60.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.60.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.60.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.60.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.60.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.60.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.60.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.60.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.60.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.60.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.61.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.61.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.61.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.61.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.61.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.61.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.61.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.61.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.61.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.61.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.61.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.61.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.62.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.62.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.62.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.62.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.62.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.62.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.62.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.62.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.62.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.62.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.62.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.62.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.63.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.63.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.63.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.63.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.63.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.63.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.63.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.63.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.63.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.63.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.63.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.63.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.64.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.64.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.64.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.64.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.64.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.64.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.64.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.64.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.64.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.64.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.64.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.64.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.65.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.65.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.65.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.65.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.65.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.65.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.65.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.65.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.65.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.65.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.65.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.65.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.66.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.66.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.66.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.66.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.66.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.66.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.66.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.66.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.66.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.66.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.66.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.66.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.67.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.67.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.67.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.67.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.67.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.67.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.67.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.67.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.67.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.67.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.67.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.67.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.68.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.68.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.68.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.68.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.68.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.68.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.68.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.68.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.68.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.68.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.68.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.68.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.69.input_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.69.input_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.69.self_attention.query_key_value.weight',\n",
              "              tensor(..., device='meta', size=(43008, 14336))),\n",
              "             ('transformer.h.69.self_attention.query_key_value.bias',\n",
              "              tensor(..., device='meta', size=(43008,))),\n",
              "             ('transformer.h.69.self_attention.dense.weight',\n",
              "              tensor(..., device='meta', size=(14336, 14336))),\n",
              "             ('transformer.h.69.self_attention.dense.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.69.post_attention_layernorm.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.69.post_attention_layernorm.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.h.69.mlp.dense_h_to_4h.weight',\n",
              "              tensor(..., device='meta', size=(57344, 14336))),\n",
              "             ('transformer.h.69.mlp.dense_h_to_4h.bias',\n",
              "              tensor(..., device='meta', size=(57344,))),\n",
              "             ('transformer.h.69.mlp.dense_4h_to_h.weight',\n",
              "              tensor(..., device='meta', size=(14336, 57344))),\n",
              "             ('transformer.h.69.mlp.dense_4h_to_h.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.ln_f.weight',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('transformer.ln_f.bias',\n",
              "              tensor(..., device='meta', size=(14336,))),\n",
              "             ('lm_head.weight',\n",
              "              tensor(..., device='meta', size=(250880, 14336)))])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***-------------------------------[2]-------------------------------***"
      ],
      "metadata": {
        "id": "UPclMXVjdrl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import infer_auto_device_map, init_empty_weights\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"facebook/opt-6.7b\", low_cpu_mem_usage=True)\n",
        "with init_empty_weights():\n",
        "    model = AutoModelForCausalLM.from_config(config)\n",
        "    \n",
        "device_map = infer_auto_device_map(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "447a2e797a3f4422adf6f536ca56bc5a",
            "f897fac9a189478eb7f56992b99aca5f",
            "81792d2dd8e8481f9200c6541eb21f33",
            "b8dc54b793644821be860c21af9636b1",
            "bacadd32d2354cde9f1580de6b083310",
            "876b4712469e4283b08dc210ee600d2a",
            "b2470fcb40cf4f14a965d22e11a88870",
            "76566b41eb1c46bfa998ea98cf5bd3f1",
            "73eff778d00c4580ac64285d2c7b4e83",
            "6feee78b9a6f4a489282e0e170e7d51f",
            "9541c8e2b2084e64aaae5e4cf30e161f"
          ]
        },
        "id": "KeRAtrnCda67",
        "outputId": "5fa6df96-cc7e-46f3-9b98-09350ae10d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "447a2e797a3f4422adf6f536ca56bc5a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656fb062-cb69-433a-a06d-5ed3630ccccc",
        "id": "cuSIHu_7da6-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 4096, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 4096)\n",
              "      (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (12): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (13): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (14): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (15): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (16): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (17): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (18): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (19): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (20): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (21): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (22): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (23): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (24): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (25): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (26): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (27): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (28): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (29): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (30): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (31): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473a1a10-32ed-4605-9b7b-e8f0bf2cb765",
        "id": "q43lKFHZda7A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model.decoder.embed_tokens': 0,\n",
              " 'model.decoder.embed_positions': 0,\n",
              " 'model.decoder.final_layer_norm': 0,\n",
              " 'model.decoder.layers.0': 0,\n",
              " 'model.decoder.layers.1': 0,\n",
              " 'model.decoder.layers.2': 0,\n",
              " 'model.decoder.layers.3': 0,\n",
              " 'model.decoder.layers.4': 0,\n",
              " 'model.decoder.layers.5': 0,\n",
              " 'model.decoder.layers.6': 0,\n",
              " 'model.decoder.layers.7': 0,\n",
              " 'model.decoder.layers.8': 0,\n",
              " 'model.decoder.layers.9': 0,\n",
              " 'model.decoder.layers.10': 0,\n",
              " 'model.decoder.layers.11': 0,\n",
              " 'model.decoder.layers.12': 0,\n",
              " 'model.decoder.layers.13': 0,\n",
              " 'model.decoder.layers.14': 0,\n",
              " 'model.decoder.layers.15': 0,\n",
              " 'model.decoder.layers.16.self_attn': 0,\n",
              " 'model.decoder.layers.16.activation_fn': 0,\n",
              " 'model.decoder.layers.16.self_attn_layer_norm': 0,\n",
              " 'model.decoder.layers.16.fc1': 0,\n",
              " 'model.decoder.layers.16.fc2': 'cpu',\n",
              " 'model.decoder.layers.16.final_layer_norm': 'cpu',\n",
              " 'model.decoder.layers.17': 'cpu',\n",
              " 'model.decoder.layers.18': 'cpu',\n",
              " 'model.decoder.layers.19': 'cpu',\n",
              " 'model.decoder.layers.20': 'cpu',\n",
              " 'model.decoder.layers.21': 'cpu',\n",
              " 'model.decoder.layers.22': 'cpu',\n",
              " 'model.decoder.layers.23': 'cpu',\n",
              " 'model.decoder.layers.24': 'cpu',\n",
              " 'model.decoder.layers.25': 'cpu',\n",
              " 'model.decoder.layers.26': 'cpu',\n",
              " 'model.decoder.layers.27': 'cpu',\n",
              " 'model.decoder.layers.28.self_attn': 'cpu',\n",
              " 'model.decoder.layers.28.activation_fn': 'cpu',\n",
              " 'model.decoder.layers.28.self_attn_layer_norm': 'cpu',\n",
              " 'model.decoder.layers.28.fc1': 'disk',\n",
              " 'model.decoder.layers.28.fc2': 'disk',\n",
              " 'model.decoder.layers.28.final_layer_norm': 'disk',\n",
              " 'model.decoder.layers.29': 'disk',\n",
              " 'model.decoder.layers.30': 'disk',\n",
              " 'model.decoder.layers.31': 'disk',\n",
              " 'lm_head': 'disk'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06716060-eee3-4508-be4a-a59db980e344",
        "id": "NOdLI9jGda7B"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('model.decoder.embed_tokens.weight',\n",
              "              tensor(..., device='meta', size=(50272, 4096))),\n",
              "             ('model.decoder.embed_positions.weight',\n",
              "              tensor(..., device='meta', size=(2050, 4096))),\n",
              "             ('model.decoder.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.0.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.0.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.0.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.0.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.0.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.0.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.0.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.0.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.0.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.0.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.0.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.0.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.0.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.0.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.0.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.0.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.1.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.1.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.1.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.1.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.1.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.1.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.1.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.1.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.1.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.1.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.1.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.1.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.1.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.1.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.1.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.1.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.2.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.2.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.2.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.2.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.2.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.2.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.2.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.2.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.2.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.2.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.2.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.2.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.2.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.2.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.2.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.2.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.3.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.3.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.3.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.3.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.3.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.3.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.3.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.3.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.3.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.3.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.3.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.3.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.3.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.3.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.3.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.3.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.4.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.4.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.4.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.4.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.4.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.4.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.4.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.4.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.4.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.4.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.4.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.4.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.4.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.4.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.4.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.4.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.5.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.5.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.5.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.5.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.5.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.5.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.5.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.5.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.5.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.5.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.5.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.5.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.5.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.5.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.5.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.5.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.6.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.6.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.6.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.6.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.6.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.6.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.6.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.6.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.6.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.6.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.6.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.6.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.6.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.6.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.6.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.6.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.7.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.7.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.7.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.7.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.7.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.7.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.7.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.7.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.7.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.7.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.7.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.7.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.7.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.7.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.7.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.7.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.8.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.8.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.8.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.8.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.8.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.8.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.8.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.8.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.8.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.8.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.8.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.8.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.8.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.8.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.8.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.8.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.9.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.9.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.9.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.9.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.9.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.9.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.9.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.9.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.9.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.9.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.9.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.9.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.9.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.9.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.9.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.9.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.10.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.10.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.10.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.10.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.10.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.10.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.10.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.10.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.10.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.10.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.10.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.10.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.10.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.10.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.10.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.10.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.11.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.11.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.11.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.11.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.11.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.11.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.11.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.11.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.11.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.11.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.11.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.11.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.11.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.11.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.11.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.11.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.12.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.12.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.12.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.12.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.12.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.12.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.12.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.12.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.12.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.12.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.12.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.12.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.12.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.12.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.12.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.12.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.13.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.13.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.13.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.13.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.13.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.13.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.13.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.13.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.13.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.13.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.13.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.13.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.13.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.13.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.13.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.13.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.14.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.14.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.14.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.14.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.14.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.14.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.14.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.14.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.14.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.14.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.14.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.14.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.14.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.14.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.14.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.14.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.15.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.15.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.15.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.15.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.15.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.15.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.15.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.15.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.15.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.15.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.15.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.15.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.15.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.15.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.15.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.15.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.16.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.16.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.16.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.16.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.16.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.16.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.16.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.16.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.16.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.16.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.16.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.16.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.16.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.16.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.16.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.16.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.17.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.17.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.17.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.17.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.17.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.17.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.17.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.17.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.17.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.17.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.17.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.17.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.17.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.17.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.17.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.17.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.18.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.18.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.18.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.18.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.18.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.18.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.18.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.18.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.18.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.18.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.18.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.18.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.18.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.18.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.18.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.18.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.19.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.19.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.19.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.19.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.19.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.19.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.19.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.19.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.19.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.19.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.19.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.19.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.19.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.19.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.19.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.19.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.20.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.20.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.20.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.20.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.20.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.20.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.20.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.20.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.20.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.20.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.20.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.20.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.20.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.20.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.20.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.20.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.21.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.21.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.21.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.21.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.21.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.21.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.21.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.21.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.21.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.21.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.21.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.21.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.21.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.21.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.21.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.21.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.22.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.22.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.22.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.22.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.22.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.22.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.22.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.22.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.22.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.22.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.22.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.22.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.22.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.22.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.22.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.22.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.23.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.23.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.23.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.23.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.23.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.23.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.23.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.23.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.23.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.23.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.23.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.23.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.23.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.23.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.23.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.23.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.24.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.24.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.24.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.24.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.24.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.24.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.24.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.24.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.24.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.24.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.24.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.24.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.24.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.24.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.24.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.24.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.25.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.25.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.25.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.25.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.25.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.25.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.25.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.25.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.25.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.25.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.25.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.25.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.25.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.25.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.25.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.25.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.26.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.26.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.26.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.26.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.26.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.26.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.26.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.26.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.26.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.26.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.26.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.26.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.26.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.26.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.26.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.26.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.27.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.27.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.27.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.27.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.27.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.27.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.27.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.27.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.27.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.27.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.27.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.27.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.27.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.27.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.27.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.27.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.28.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.28.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.28.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.28.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.28.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.28.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.28.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.28.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.28.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.28.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.28.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.28.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.28.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.28.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.28.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.28.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.29.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.29.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.29.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.29.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.29.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.29.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.29.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.29.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.29.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.29.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.29.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.29.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.29.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.29.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.29.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.29.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.30.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.30.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.30.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.30.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.30.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.30.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.30.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.30.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.30.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.30.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.30.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.30.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.30.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.30.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.30.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.30.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.31.self_attn.k_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.31.self_attn.k_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.31.self_attn.v_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.31.self_attn.v_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.31.self_attn.q_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.31.self_attn.q_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.31.self_attn.out_proj.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('model.decoder.layers.31.self_attn.out_proj.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.31.self_attn_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.31.self_attn_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.31.fc1.weight',\n",
              "              tensor(..., device='meta', size=(16384, 4096))),\n",
              "             ('model.decoder.layers.31.fc1.bias',\n",
              "              tensor(..., device='meta', size=(16384,))),\n",
              "             ('model.decoder.layers.31.fc2.weight',\n",
              "              tensor(..., device='meta', size=(4096, 16384))),\n",
              "             ('model.decoder.layers.31.fc2.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.31.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('model.decoder.layers.31.final_layer_norm.bias',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('lm_head.weight',\n",
              "              tensor(..., device='meta', size=(50272, 4096)))])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***-------------------------------[3]-------------------------------***"
      ],
      "metadata": {
        "id": "kFKX6Axjd54X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import infer_auto_device_map, init_empty_weights\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"facebook/opt-13b\", low_cpu_mem_usage=True)\n",
        "with init_empty_weights():\n",
        "    model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "device_map = infer_auto_device_map(model)\n"
      ],
      "metadata": {
        "id": "T9a_tnDXRB5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfPVPSl3Sk3r",
        "outputId": "2a8494e8-163f-4d81-9652-6882da191518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 5120, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)\n",
              "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (12): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (13): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (14): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (15): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (16): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (17): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (18): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (19): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (20): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (21): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (22): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (23): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (24): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (25): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (26): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (27): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (28): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (29): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (30): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (31): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (32): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (33): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (34): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (35): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (36): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (37): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (38): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (39): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=5120, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEsoJph6SiB7",
        "outputId": "52cc9dc7-5faf-4181-ac18-b21109a5a62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model.decoder.embed_tokens': 0,\n",
              " 'model.decoder.embed_positions': 0,\n",
              " 'model.decoder.final_layer_norm': 0,\n",
              " 'model.decoder.layers.0': 0,\n",
              " 'model.decoder.layers.1': 0,\n",
              " 'model.decoder.layers.2': 0,\n",
              " 'model.decoder.layers.3': 0,\n",
              " 'model.decoder.layers.4': 0,\n",
              " 'model.decoder.layers.5': 0,\n",
              " 'model.decoder.layers.6': 0,\n",
              " 'model.decoder.layers.7': 0,\n",
              " 'model.decoder.layers.8': 0,\n",
              " 'model.decoder.layers.9': 0,\n",
              " 'model.decoder.layers.10.self_attn': 0,\n",
              " 'model.decoder.layers.10.activation_fn': 0,\n",
              " 'model.decoder.layers.10.self_attn_layer_norm': 0,\n",
              " 'model.decoder.layers.10.fc1': 'cpu',\n",
              " 'model.decoder.layers.10.fc2': 'cpu',\n",
              " 'model.decoder.layers.10.final_layer_norm': 'cpu',\n",
              " 'model.decoder.layers.11': 'cpu',\n",
              " 'model.decoder.layers.12': 'cpu',\n",
              " 'model.decoder.layers.13': 'cpu',\n",
              " 'model.decoder.layers.14': 'cpu',\n",
              " 'model.decoder.layers.15': 'cpu',\n",
              " 'model.decoder.layers.16': 'cpu',\n",
              " 'model.decoder.layers.17.self_attn': 'cpu',\n",
              " 'model.decoder.layers.17.activation_fn': 'cpu',\n",
              " 'model.decoder.layers.17.self_attn_layer_norm': 'cpu',\n",
              " 'model.decoder.layers.17.fc1': 'cpu',\n",
              " 'model.decoder.layers.17.fc2': 'disk',\n",
              " 'model.decoder.layers.17.final_layer_norm': 'disk',\n",
              " 'model.decoder.layers.18': 'disk',\n",
              " 'model.decoder.layers.19': 'disk',\n",
              " 'model.decoder.layers.20': 'disk',\n",
              " 'model.decoder.layers.21': 'disk',\n",
              " 'model.decoder.layers.22': 'disk',\n",
              " 'model.decoder.layers.23': 'disk',\n",
              " 'model.decoder.layers.24': 'disk',\n",
              " 'model.decoder.layers.25': 'disk',\n",
              " 'model.decoder.layers.26': 'disk',\n",
              " 'model.decoder.layers.27': 'disk',\n",
              " 'model.decoder.layers.28': 'disk',\n",
              " 'model.decoder.layers.29': 'disk',\n",
              " 'model.decoder.layers.30': 'disk',\n",
              " 'model.decoder.layers.31': 'disk',\n",
              " 'model.decoder.layers.32': 'disk',\n",
              " 'model.decoder.layers.33': 'disk',\n",
              " 'model.decoder.layers.34': 'disk',\n",
              " 'model.decoder.layers.35': 'disk',\n",
              " 'model.decoder.layers.36': 'disk',\n",
              " 'model.decoder.layers.37': 'disk',\n",
              " 'model.decoder.layers.38': 'disk',\n",
              " 'model.decoder.layers.39': 'disk',\n",
              " 'lm_head': 'disk'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY7UUTVPfW40",
        "outputId": "671c0701-7038-43d2-e584-9f4330499ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.state_dict of OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 5120, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)\n",
              "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (12): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (13): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (14): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (15): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (16): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (17): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (18): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (19): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (20): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (21): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (22): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (23): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (24): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (25): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (26): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (27): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (28): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (29): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (30): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (31): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (32): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (33): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (34): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (35): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (36): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (37): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (38): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (39): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
              "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
              "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=5120, out_features=50272, bias=False)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.architectures"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv9HgQkKfhhl",
        "outputId": "15c7cf84-4853-45dd-f2bc-0a91ea729f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['OPTForCausalLM']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bis_9i9tfrF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "#t0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)"
      ],
      "metadata": {
        "id": "NqmCYhNXZ3Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***-------------------------------[4]-------------------------------***"
      ],
      "metadata": {
        "id": "nAhT2M8ker7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import infer_auto_device_map, init_empty_weights\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"google/flan-t5-xxl\", low_cpu_mem_usage=True)\n",
        "with init_empty_weights():\n",
        "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
        "\n",
        "device_map = infer_auto_device_map(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "cG2GCoAXTUMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRbKU_9ITvYK",
        "outputId": "a311cce4-a460-4174-95af-c0987e9518e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 4096)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 4096)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 64)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (17): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (18): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (19): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (20): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (21): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (22): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (23): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 4096)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 64)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (17): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (18): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (19): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (20): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (21): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (22): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (23): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PP0zD4uUDDj",
        "outputId": "583d760c-c121-465a-8b04-48244cd51e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'shared': 0,\n",
              " 'decoder.embed_tokens': 0,\n",
              " 'encoder.embed_tokens': 0,\n",
              " 'encoder.block.0': 0,\n",
              " 'encoder.block.1': 0,\n",
              " 'encoder.block.2': 0,\n",
              " 'encoder.block.3': 0,\n",
              " 'encoder.block.4': 0,\n",
              " 'encoder.block.5': 0,\n",
              " 'encoder.block.6': 0,\n",
              " 'encoder.block.7': 0,\n",
              " 'encoder.block.8': 0,\n",
              " 'encoder.block.9': 0,\n",
              " 'encoder.block.10': 0,\n",
              " 'encoder.block.11': 0,\n",
              " 'encoder.block.12': 0,\n",
              " 'encoder.block.13': 0,\n",
              " 'encoder.block.14': 0,\n",
              " 'encoder.block.15': 0,\n",
              " 'encoder.block.16': 0,\n",
              " 'encoder.block.17.layer.0': 0,\n",
              " 'encoder.block.17.layer.1.DenseReluDense.wi_0': 0,\n",
              " 'encoder.block.17.layer.1.DenseReluDense.wi_1': 'cpu',\n",
              " 'encoder.block.17.layer.1.DenseReluDense.wo': 'cpu',\n",
              " 'encoder.block.17.layer.1.DenseReluDense.dropout': 'cpu',\n",
              " 'encoder.block.17.layer.1.DenseReluDense.act': 'cpu',\n",
              " 'encoder.block.17.layer.1.layer_norm': 'cpu',\n",
              " 'encoder.block.17.layer.1.dropout': 'cpu',\n",
              " 'encoder.block.18': 'cpu',\n",
              " 'encoder.block.19': 'cpu',\n",
              " 'encoder.block.20': 'cpu',\n",
              " 'encoder.block.21': 'cpu',\n",
              " 'encoder.block.22': 'cpu',\n",
              " 'encoder.block.23': 'cpu',\n",
              " 'encoder.final_layer_norm': 'cpu',\n",
              " 'encoder.dropout': 'cpu',\n",
              " 'decoder.block.0': 'cpu',\n",
              " 'decoder.block.1': 'cpu',\n",
              " 'decoder.block.2': 'cpu',\n",
              " 'decoder.block.3': 'cpu',\n",
              " 'decoder.block.4.layer.0': 'cpu',\n",
              " 'decoder.block.4.layer.1.EncDecAttention.q': 'cpu',\n",
              " 'decoder.block.4.layer.1.EncDecAttention.k': 'disk',\n",
              " 'decoder.block.4.layer.1.EncDecAttention.v': 'disk',\n",
              " 'decoder.block.4.layer.1.EncDecAttention.o': 'disk',\n",
              " 'decoder.block.4.layer.1.layer_norm': 'disk',\n",
              " 'decoder.block.4.layer.1.dropout': 'disk',\n",
              " 'decoder.block.4.layer.2': 'disk',\n",
              " 'decoder.block.5': 'disk',\n",
              " 'decoder.block.6': 'disk',\n",
              " 'decoder.block.7': 'disk',\n",
              " 'decoder.block.8': 'disk',\n",
              " 'decoder.block.9': 'disk',\n",
              " 'decoder.block.10': 'disk',\n",
              " 'decoder.block.11': 'disk',\n",
              " 'decoder.block.12': 'disk',\n",
              " 'decoder.block.13': 'disk',\n",
              " 'decoder.block.14': 'disk',\n",
              " 'decoder.block.15': 'disk',\n",
              " 'decoder.block.16': 'disk',\n",
              " 'decoder.block.17': 'disk',\n",
              " 'decoder.block.18': 'disk',\n",
              " 'decoder.block.19': 'disk',\n",
              " 'decoder.block.20': 'disk',\n",
              " 'decoder.block.21': 'disk',\n",
              " 'decoder.block.22': 'disk',\n",
              " 'decoder.block.23': 'disk',\n",
              " 'decoder.final_layer_norm': 'disk',\n",
              " 'decoder.dropout': 'disk',\n",
              " 'lm_head': 'disk'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XumhU9ZgZKeY",
        "outputId": "ba165be7-950d-45b0-8cf2-c874e2e10951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('shared.weight', tensor(..., device='meta', size=(32128, 4096))),\n",
              "             ('encoder.embed_tokens.weight',\n",
              "              tensor(..., device='meta', size=(32128, 4096))),\n",
              "             ('encoder.block.0.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.0.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.0.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.0.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
              "              tensor(..., device='meta', size=(32, 64))),\n",
              "             ('encoder.block.0.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.0.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.0.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.0.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.0.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.1.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.1.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.1.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.1.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.1.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.1.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.1.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.1.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.1.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.2.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.2.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.2.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.2.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.2.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.2.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.2.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.2.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.2.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.3.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.3.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.3.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.3.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.3.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.3.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.3.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.3.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.3.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.4.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.4.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.4.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.4.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.4.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.4.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.4.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.4.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.4.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.5.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.5.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.5.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.5.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.5.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.5.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.5.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.5.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.5.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.6.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.6.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.6.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.6.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.6.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.6.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.6.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.6.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.6.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.7.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.7.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.7.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.7.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.7.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.7.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.7.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.7.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.7.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.8.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.8.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.8.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.8.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.8.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.8.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.8.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.8.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.8.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.9.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.9.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.9.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.9.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.9.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.9.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.9.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.9.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.9.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.10.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.10.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.10.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.10.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.10.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.10.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.10.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.10.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.10.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.11.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.11.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.11.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.11.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.11.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.11.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.11.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.11.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.11.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.12.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.12.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.12.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.12.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.12.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.12.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.12.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.12.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.12.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.13.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.13.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.13.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.13.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.13.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.13.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.13.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.13.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.13.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.14.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.14.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.14.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.14.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.14.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.14.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.14.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.14.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.14.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.15.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.15.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.15.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.15.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.15.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.15.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.15.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.15.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.15.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.16.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.16.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.16.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.16.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.16.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.16.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.16.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.16.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.16.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.17.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.17.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.17.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.17.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.17.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.17.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.17.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.17.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.17.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.18.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.18.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.18.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.18.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.18.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.18.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.18.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.18.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.18.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.19.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.19.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.19.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.19.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.19.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.19.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.19.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.19.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.19.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.20.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.20.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.20.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.20.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.20.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.20.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.20.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.20.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.20.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.21.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.21.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.21.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.21.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.21.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.21.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.21.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.21.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.21.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.22.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.22.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.22.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.22.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.22.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.22.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.22.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.22.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.22.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.23.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.23.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.23.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.23.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.23.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.23.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.23.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.23.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.23.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.embed_tokens.weight',\n",
              "              tensor(..., device='meta', size=(32128, 4096))),\n",
              "             ('decoder.block.0.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
              "              tensor(..., device='meta', size=(32, 64))),\n",
              "             ('decoder.block.0.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.0.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.0.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.0.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.0.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.0.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.1.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.1.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.1.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.1.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.1.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.1.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.2.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.2.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.2.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.2.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.2.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.2.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.3.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.3.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.3.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.3.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.3.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.3.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.4.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.4.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.4.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.4.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.4.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.4.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.5.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.5.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.5.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.5.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.5.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.5.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.6.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.6.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.6.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.6.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.6.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.6.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.7.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.7.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.7.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.7.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.7.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.7.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.8.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.8.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.8.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.8.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.8.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.8.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.9.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.9.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.9.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.9.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.9.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.9.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.10.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.10.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.10.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.10.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.10.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.10.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.11.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.11.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.11.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.11.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.11.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.11.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.12.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.12.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.12.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.12.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.12.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.12.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.13.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.13.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.13.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.13.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.13.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.13.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.14.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.14.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.14.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.14.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.14.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.14.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.15.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.15.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.15.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.15.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.15.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.15.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.16.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.16.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.16.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.16.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.16.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.16.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.17.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.17.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.17.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.17.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.17.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.17.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.18.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.18.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.18.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.18.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.18.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.18.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.19.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.19.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.19.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.19.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.19.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.19.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.20.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.20.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.20.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.20.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.20.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.20.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.21.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.21.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.21.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.21.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.21.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.21.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.22.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.22.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.22.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.22.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.22.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.22.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.23.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.23.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.23.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.23.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.23.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.23.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('lm_head.weight',\n",
              "              tensor(..., device='meta', size=(32128, 4096)))])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model1.load_state_dict(model2.state_dict())"
      ],
      "metadata": {
        "id": "OJrrc_ixZcpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***-------------------------------[5]-------------------------------***"
      ],
      "metadata": {
        "id": "zdTITCkBe1pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import infer_auto_device_map, init_empty_weights\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)\n",
        "\n",
        "with init_empty_weights():\n",
        "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
        "\n",
        "device_map = infer_auto_device_map(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "VRdCHIEHaBRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-fxBPlyZdV_",
        "outputId": "40c72bd0-9aad-4713-b695-b2c419d6ca53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 4096)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 4096)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 64)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (17): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (18): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (19): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (20): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (21): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (22): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (23): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 4096)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 64)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (17): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (18): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (19): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (20): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (21): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (22): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (23): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N0_uFfjbDmK",
        "outputId": "f97614d3-22fd-4b17-8735-a922c965ba51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'shared': 0,\n",
              " 'decoder.embed_tokens': 0,\n",
              " 'encoder.embed_tokens': 0,\n",
              " 'encoder.block.0': 0,\n",
              " 'encoder.block.1': 0,\n",
              " 'encoder.block.2': 0,\n",
              " 'encoder.block.3': 0,\n",
              " 'encoder.block.4': 0,\n",
              " 'encoder.block.5': 0,\n",
              " 'encoder.block.6': 0,\n",
              " 'encoder.block.7': 0,\n",
              " 'encoder.block.8': 0,\n",
              " 'encoder.block.9': 0,\n",
              " 'encoder.block.10': 0,\n",
              " 'encoder.block.11': 0,\n",
              " 'encoder.block.12': 0,\n",
              " 'encoder.block.13': 0,\n",
              " 'encoder.block.14': 0,\n",
              " 'encoder.block.15': 0,\n",
              " 'encoder.block.16': 0,\n",
              " 'encoder.block.17': 0,\n",
              " 'encoder.block.18.layer.0.SelfAttention.q': 0,\n",
              " 'encoder.block.18.layer.0.SelfAttention.k': 0,\n",
              " 'encoder.block.18.layer.0.SelfAttention.v': 0,\n",
              " 'encoder.block.18.layer.0.SelfAttention.o': 'cpu',\n",
              " 'encoder.block.18.layer.0.layer_norm': 'cpu',\n",
              " 'encoder.block.18.layer.0.dropout': 'cpu',\n",
              " 'encoder.block.18.layer.1': 'cpu',\n",
              " 'encoder.block.19': 'cpu',\n",
              " 'encoder.block.20': 'cpu',\n",
              " 'encoder.block.21': 'cpu',\n",
              " 'encoder.block.22': 'cpu',\n",
              " 'encoder.block.23': 'cpu',\n",
              " 'encoder.final_layer_norm': 'cpu',\n",
              " 'encoder.dropout': 'cpu',\n",
              " 'decoder.block.0': 'cpu',\n",
              " 'decoder.block.1': 'cpu',\n",
              " 'decoder.block.2': 'cpu',\n",
              " 'decoder.block.3': 'cpu',\n",
              " 'decoder.block.4': 'cpu',\n",
              " 'decoder.block.5.layer.0.SelfAttention.q': 'cpu',\n",
              " 'decoder.block.5.layer.0.SelfAttention.k': 'cpu',\n",
              " 'decoder.block.5.layer.0.SelfAttention.v': 'cpu',\n",
              " 'decoder.block.5.layer.0.SelfAttention.o': 'disk',\n",
              " 'decoder.block.5.layer.0.layer_norm': 'disk',\n",
              " 'decoder.block.5.layer.0.dropout': 'disk',\n",
              " 'decoder.block.5.layer.1': 'disk',\n",
              " 'decoder.block.5.layer.2': 'disk',\n",
              " 'decoder.block.6': 'disk',\n",
              " 'decoder.block.7': 'disk',\n",
              " 'decoder.block.8': 'disk',\n",
              " 'decoder.block.9': 'disk',\n",
              " 'decoder.block.10': 'disk',\n",
              " 'decoder.block.11': 'disk',\n",
              " 'decoder.block.12': 'disk',\n",
              " 'decoder.block.13': 'disk',\n",
              " 'decoder.block.14': 'disk',\n",
              " 'decoder.block.15': 'disk',\n",
              " 'decoder.block.16': 'disk',\n",
              " 'decoder.block.17': 'disk',\n",
              " 'decoder.block.18': 'disk',\n",
              " 'decoder.block.19': 'disk',\n",
              " 'decoder.block.20': 'disk',\n",
              " 'decoder.block.21': 'disk',\n",
              " 'decoder.block.22': 'disk',\n",
              " 'decoder.block.23': 'disk',\n",
              " 'decoder.final_layer_norm': 'disk',\n",
              " 'decoder.dropout': 'disk',\n",
              " 'lm_head': 'disk'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgxf6BYgbNDj",
        "outputId": "c4011024-84a8-47a2-bc72-086bfe5b8184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('shared.weight', tensor(..., device='meta', size=(32128, 4096))),\n",
              "             ('encoder.embed_tokens.weight',\n",
              "              tensor(..., device='meta', size=(32128, 4096))),\n",
              "             ('encoder.block.0.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.0.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.0.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.0.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
              "              tensor(..., device='meta', size=(32, 64))),\n",
              "             ('encoder.block.0.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.0.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.0.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.0.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.0.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.1.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.1.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.1.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.1.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.1.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.1.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.1.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.1.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.1.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.2.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.2.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.2.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.2.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.2.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.2.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.2.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.2.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.2.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.3.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.3.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.3.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.3.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.3.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.3.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.3.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.3.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.3.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.4.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.4.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.4.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.4.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.4.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.4.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.4.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.4.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.4.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.5.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.5.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.5.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.5.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.5.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.5.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.5.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.5.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.5.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.6.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.6.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.6.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.6.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.6.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.6.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.6.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.6.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.6.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.7.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.7.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.7.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.7.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.7.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.7.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.7.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.7.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.7.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.8.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.8.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.8.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.8.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.8.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.8.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.8.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.8.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.8.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.9.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.9.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.9.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.9.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.9.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.9.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.9.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.9.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.9.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.10.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.10.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.10.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.10.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.10.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.10.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.10.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.10.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.10.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.11.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.11.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.11.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.11.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.11.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.11.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.11.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.11.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.11.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.12.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.12.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.12.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.12.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.12.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.12.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.12.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.12.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.12.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.13.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.13.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.13.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.13.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.13.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.13.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.13.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.13.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.13.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.14.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.14.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.14.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.14.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.14.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.14.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.14.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.14.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.14.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.15.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.15.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.15.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.15.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.15.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.15.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.15.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.15.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.15.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.16.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.16.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.16.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.16.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.16.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.16.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.16.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.16.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.16.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.17.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.17.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.17.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.17.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.17.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.17.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.17.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.17.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.17.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.18.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.18.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.18.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.18.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.18.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.18.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.18.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.18.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.18.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.19.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.19.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.19.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.19.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.19.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.19.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.19.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.19.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.19.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.20.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.20.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.20.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.20.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.20.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.20.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.20.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.20.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.20.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.21.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.21.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.21.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.21.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.21.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.21.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.21.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.21.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.21.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.22.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.22.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.22.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.22.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.22.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.22.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.22.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.22.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.22.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.23.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.23.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.23.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.23.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('encoder.block.23.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.block.23.layer.1.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.23.layer.1.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('encoder.block.23.layer.1.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('encoder.block.23.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('encoder.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.embed_tokens.weight',\n",
              "              tensor(..., device='meta', size=(32128, 4096))),\n",
              "             ('decoder.block.0.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
              "              tensor(..., device='meta', size=(32, 64))),\n",
              "             ('decoder.block.0.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.0.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.0.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.0.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.0.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.0.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.0.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.1.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.1.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.1.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.1.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.1.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.1.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.1.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.2.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.2.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.2.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.2.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.2.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.2.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.2.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.3.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.3.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.3.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.3.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.3.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.3.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.3.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.4.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.4.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.4.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.4.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.4.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.4.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.4.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.5.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.5.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.5.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.5.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.5.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.5.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.5.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.6.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.6.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.6.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.6.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.6.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.6.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.6.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.7.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.7.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.7.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.7.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.7.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.7.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.7.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.8.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.8.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.8.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.8.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.8.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.8.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.8.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.9.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.9.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.9.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.9.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.9.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.9.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.9.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.10.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.10.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.10.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.10.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.10.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.10.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.10.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.11.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.11.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.11.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.11.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.11.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.11.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.11.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.12.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.12.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.12.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.12.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.12.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.12.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.12.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.13.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.13.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.13.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.13.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.13.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.13.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.13.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.14.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.14.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.14.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.14.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.14.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.14.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.14.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.15.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.15.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.15.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.15.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.15.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.15.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.15.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.16.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.16.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.16.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.16.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.16.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.16.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.16.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.17.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.17.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.17.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.17.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.17.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.17.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.17.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.18.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.18.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.18.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.18.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.18.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.18.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.18.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.19.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.19.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.19.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.19.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.19.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.19.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.19.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.20.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.20.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.20.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.20.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.20.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.20.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.20.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.21.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.21.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.21.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.21.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.21.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.21.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.21.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.22.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.22.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.22.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.22.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.22.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.22.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.22.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.23.layer.0.SelfAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.0.SelfAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.0.SelfAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.0.SelfAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.0.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.23.layer.1.EncDecAttention.q.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.1.EncDecAttention.k.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.1.EncDecAttention.v.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.1.EncDecAttention.o.weight',\n",
              "              tensor(..., device='meta', size=(4096, 4096))),\n",
              "             ('decoder.block.23.layer.1.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.block.23.layer.2.DenseReluDense.wi_0.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.23.layer.2.DenseReluDense.wi_1.weight',\n",
              "              tensor(..., device='meta', size=(10240, 4096))),\n",
              "             ('decoder.block.23.layer.2.DenseReluDense.wo.weight',\n",
              "              tensor(..., device='meta', size=(4096, 10240))),\n",
              "             ('decoder.block.23.layer.2.layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('decoder.final_layer_norm.weight',\n",
              "              tensor(..., device='meta', size=(4096,))),\n",
              "             ('lm_head.weight',\n",
              "              tensor(..., device='meta', size=(32128, 4096)))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM**"
      ],
      "metadata": {
        "id": "wSA_uxR7zUYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "#This code load all the model weights\n",
        "\"\"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cerebras/Cerebras-GPT-13B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"cerebras/Cerebras-GPT-13B\")\n",
        "\n",
        "text = \"Generative AI is \"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vgnRkrS7bMqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import infer_auto_device_map, init_empty_weights\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"cerebras/Cerebras-GPT-13B\", low_cpu_mem_usage=True)\n",
        "\n",
        "with init_empty_weights():\n",
        "    model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "device_map = infer_auto_device_map(model)"
      ],
      "metadata": {
        "id": "uyHkpzhtULaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e342ab1c4b2d4c5399d45da305cf19dc",
            "3419cef609194fca86834b08ec555837",
            "df8d94c8e3904d19bc3bd081f6018b5c",
            "fda2b81df2094eeab7a916c4617655a0",
            "03f616b01c4c4405a6e1f3fe6d93a918",
            "b8a909e43db945f4b9d50a60bc2ca4c8",
            "cc7c6b8c75aa432d954a45a48efa754f",
            "353c8a995d6847c7a575ed967acf78e5",
            "61df1e0ad3e54adca7f72dd349c78e72",
            "97cea7d5720a4eb08b2343ca42983581",
            "cdb699f8d1ba4df4b731c3422c203057"
          ]
        },
        "outputId": "5fa74605-538c-4f07-da86-45e46d83f8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/800 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e342ab1c4b2d4c5399d45da305cf19dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw33Eo_Qzj99",
        "outputId": "602dc34a-bbe5-49df-bc24-52f23a2462e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 5120)\n",
              "    (wpe): Embedding(2048, 5120)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (24): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (25): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (26): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (27): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (28): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (29): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (30): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (31): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (32): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (33): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (34): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (35): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (36): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (37): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (38): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (39): GPT2Block(\n",
              "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): GELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=5120, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5qSsKTqzjhz",
        "outputId": "a7a8edc5-7657-4c54-b83d-2c81d4e6ad35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'transformer.wte': 'cpu',\n",
              " 'transformer.wpe': 'cpu',\n",
              " 'transformer.drop': 'cpu',\n",
              " 'transformer.h.0': 'cpu',\n",
              " 'transformer.h.1': 'cpu',\n",
              " 'transformer.h.2': 'cpu',\n",
              " 'transformer.h.3': 'cpu',\n",
              " 'transformer.h.4': 'cpu',\n",
              " 'transformer.h.5': 'cpu',\n",
              " 'transformer.h.6': 'cpu',\n",
              " 'transformer.h.7.ln_1': 'cpu',\n",
              " 'transformer.h.7.attn': 'cpu',\n",
              " 'transformer.h.7.ln_2': 'cpu',\n",
              " 'transformer.h.8': 'disk',\n",
              " 'transformer.h.9': 'disk',\n",
              " 'transformer.h.10': 'disk',\n",
              " 'transformer.h.11': 'disk',\n",
              " 'transformer.h.12': 'disk',\n",
              " 'transformer.h.13': 'disk',\n",
              " 'transformer.h.14': 'disk',\n",
              " 'transformer.h.15': 'disk',\n",
              " 'transformer.h.16': 'disk',\n",
              " 'transformer.h.17': 'disk',\n",
              " 'transformer.h.18': 'disk',\n",
              " 'transformer.h.19': 'disk',\n",
              " 'transformer.h.20': 'disk',\n",
              " 'transformer.h.21': 'disk',\n",
              " 'transformer.h.22': 'disk',\n",
              " 'transformer.h.23': 'disk',\n",
              " 'transformer.h.24': 'disk',\n",
              " 'transformer.h.25': 'disk',\n",
              " 'transformer.h.26': 'disk',\n",
              " 'transformer.h.27': 'disk',\n",
              " 'transformer.h.28': 'disk',\n",
              " 'transformer.h.29': 'disk',\n",
              " 'transformer.h.30': 'disk',\n",
              " 'transformer.h.31': 'disk',\n",
              " 'transformer.h.32': 'disk',\n",
              " 'transformer.h.33': 'disk',\n",
              " 'transformer.h.34': 'disk',\n",
              " 'transformer.h.35': 'disk',\n",
              " 'transformer.h.36': 'disk',\n",
              " 'transformer.h.37': 'disk',\n",
              " 'transformer.h.38': 'disk',\n",
              " 'transformer.h.39': 'disk',\n",
              " 'transformer.ln_f': 'disk',\n",
              " 'lm_head': 'disk',\n",
              " 'transformer.h.7.mlp': 'disk'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_memlab import profile, set_target_gpu\n",
        "@profile\n",
        "def func():\n",
        "    net1 = torch.nn.Linear(1024, 1024)\n",
        "    set_target_gpu(1)\n",
        "    net2 = torch.nn.Linear(1024, 1024)\n",
        "    set_target_gpu(0)\n",
        "    net3 = torch.nn.Linear(1024, 1024)\n",
        "\n",
        "func()"
      ],
      "metadata": {
        "id": "K37aAGIA1hzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"torch.distributed.reduce_op is deprecated\")"
      ],
      "metadata": {
        "id": "gpfc74PC7dBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i2IrLZWKZQfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Various**"
      ],
      "metadata": {
        "id": "UlKci8t-Y0Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "#t0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)"
      ],
      "metadata": {
        "id": "KWNqa7lDD7kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "#t0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\")\n",
        "#t0pp.hf_device_map"
      ],
      "metadata": {
        "id": "z5tMOSijEmFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "from accelerate import accelerator "
      ],
      "metadata": {
        "id": "AfnS-a9vE-81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cht8Y2gHGmJ8"
      },
      "outputs": [],
      "source": [
        "from safetensors import safe_open\n",
        "\n",
        "tensors = {}\n",
        "with safe_open(\"model.safetensors\", framework=\"pt\", device=0) as f:\n",
        "    for k in f.keys():\n",
        "        tensors[k] = f.get_tensor(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDjO3QYyGmKC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCB1_-xeGmKI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b7d19786c6b24f869f606a44c3977d8d",
            "e597290bb22c4ca28d6e2705d67873da",
            "a7781311293b44c582ecfa3eed84b138",
            "8421a990c879401e98dc9a282f0fb450",
            "8daf564769f54e558868e1d6def7ae62",
            "3dd3df2deb0043f7aac68852a80d1ac2",
            "e41586371b8040f8959aa24984910d96",
            "aff60ff9656246acb05f4614fb2e8140",
            "004f6a1abad84907bfa11ae1f09005de",
            "faffe06613e843c396f7fc0e705cfe28",
            "7e8b290ce731473dad7d219828d1a852"
          ]
        },
        "outputId": "2c5e2311-ea3d-4caf-f35c-e65ea376d5eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7d19786c6b24f869f606a44c3977d8d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "sf_filename = hf_hub_download(\"gpt2\", filename=\"model.safetensors\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sf_filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "uTWkFtX-HBYI",
        "outputId": "956d2e22-957f-40ea-9b21-74cc1180f40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/model.safetensors'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pt_filename = hf_hub_download(\"gpt2\", filename=\"pytorch_model.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "422d91a0bf244c3aa8433b6c1d10a174",
            "73b8d828ba544fb2ab8701455da8cf40",
            "748a9e109eb0478fa8d3c6cf671ab2ee",
            "a65d326ace3642a7bfaf157423e3cf03",
            "fc2a929e7b8f4fe19c317ed31850dcc3",
            "77f9b1cad4c64ae6bd2a032b51899966",
            "475d714b4035471483ff7267aacc9da6",
            "4aea1d234dad4842bc222df94b4c985d",
            "fdb4001a4374465188978357637c103c",
            "ff43bca06d084edbb1559af3c0c93c60",
            "6ccfbe669fd64ebc8ac26cff6d7693e3"
          ]
        },
        "id": "o1Ua-5naHAg1",
        "outputId": "f18230f2-e713-44b3-f43f-e59d782ab970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "422d91a0bf244c3aa8433b6c1d10a174"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pt_filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "jsMYdhtTHX3-",
        "outputId": "3455d017-cda9-49b4-b6b6-272173321505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70DjzL5-GmKT",
        "outputId": "c5475ac6-aead-4a66-fb3e-abd247bdfab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded safetensors 0:00:00.055665\n",
            "Loaded pytorch 0:00:00.306339\n",
            "on CPU, safetensors is faster than pytorch by: 5.5 X\n"
          ]
        }
      ],
      "source": [
        "start_st = datetime.datetime.now()\n",
        "weights = load_file(sf_filename, device=\"cpu\")\n",
        "load_time_st = datetime.datetime.now() - start_st\n",
        "print(f\"Loaded safetensors {load_time_st}\")\n",
        "\n",
        "start_pt = datetime.datetime.now()\n",
        "weights = torch.load(pt_filename, map_location=\"cpu\")\n",
        "load_time_pt = datetime.datetime.now() - start_pt\n",
        "print(f\"Loaded pytorch {load_time_pt}\")\n",
        "\n",
        "print(f\"on CPU, safetensors is faster than pytorch by: {load_time_pt/load_time_st:.1f} X\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--OQkWEmGmKc",
        "outputId": "cf7b7d22-4c61-4c47-901d-e3bee391d0ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded safetensors 0:00:00.174463\n",
            "Loaded pytorch 0:00:00.316121\n",
            "on GPU, safetensors is faster than pytorch by: 1.8 X\n"
          ]
        }
      ],
      "source": [
        "# This is required because this feature hasn't been fully verified yet, but \n",
        "# it's been tested on many different environments\n",
        "os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "# CUDA startup out of the measurement\n",
        "torch.zeros((2, 2)).cuda()\n",
        "\n",
        "start_st = datetime.datetime.now()\n",
        "weights = load_file(sf_filename, device=\"cuda:0\")\n",
        "load_time_st = datetime.datetime.now() - start_st\n",
        "print(f\"Loaded safetensors {load_time_st}\")\n",
        "\n",
        "start_pt = datetime.datetime.now()\n",
        "weights = torch.load(pt_filename, map_location=\"cuda:0\")\n",
        "load_time_pt = datetime.datetime.now() - start_pt\n",
        "print(f\"Loaded pytorch {load_time_pt}\")\n",
        "\n",
        "print(f\"on GPU, safetensors is faster than pytorch by: {load_time_pt/load_time_st:.1f} X\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wAl7qxFEQ5Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# This works on a base Colab instance.\n",
        "# Pick a larger checkpoint if you have time to wait and enough disk space!\n",
        "checkpoint = \"facebook/opt-6.7b\"\n",
        "generator = pipeline(\"text-generation\", model=checkpoint, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "# Perform inference\n",
        "generator(\"More and more large language models are opensourced so Hugging Face has\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613,
          "referenced_widgets": [
            "5b82c1ac52f64d31a60a33ef85c34994",
            "58c62861f6634204b82c11e14a62cd7b",
            "d18732a49bec4a18bd0db58f948cb7f0",
            "bf01be65a95740b29995371d30e10b4e",
            "56ef6220e5a543c5a41b045a13afabf0",
            "a84c60444632481d98b8dfbb8d640c89",
            "5f769de6ee844593a66aea52ccf3d422",
            "dedfb782b19843f2b48dc8ea7d4e5c88",
            "11cd9fdecf1649b28930777b503a789a",
            "3b481d5824424ef6ae2cc2afb5f47c1e",
            "7785884590e645dd9b94f956406ba6e0",
            "f6582a57b7724a2d8ed49882e604d2b2",
            "becae544e800441eac6de5f6b27ece51",
            "d1c12081b8d449898c15685054f6de34",
            "20c24346fb974c8c891fee9e15343bdd",
            "a9dfcc5a23fa45ee85bbf12c821e1471",
            "2bda1dae10714de3bcdeff2e4def383b",
            "a6872e723fcc4e6ba96696dfa9e16954",
            "82cca18689984eceb6f5c7f694411db0",
            "61ac5cf95013493ba91133e411b3958f",
            "630d32402aa04cb49c1cdb38605f5af6",
            "9c33879bc28d4cecb7b28ae10184baab",
            "c8ff2afd81fe4207bf62eadd256dca0f",
            "ea14219ed8d24205b0729bdf3b04fd0c",
            "4aeb13c31ca54d22a706dc2eb95f05a1",
            "4d161d390b6147cb9906eecc53bfbbdc",
            "da1349df4be84311b075f7656a7f1ce9",
            "cd00c594c4514ec7bd38bcb987c04063",
            "8525eaf0029c45689e527b1ee65221b6",
            "1a8ad8ffc0264164af7f23c3b7328642",
            "e41d3160516a4111886fb96532bb288f",
            "45927736f8f5402881f33e157731a27a",
            "1901a07c48c2454e8461644ec0e8984a",
            "1a60a8a2d20a4b6fb0f3e791d55dfaa8",
            "ed1cf706337f4a44bcd4e0478d00a949",
            "d555cdfcf77f4b4bbd6241960eacfaa7",
            "45a88bca1a654dc2af56bcfe50659d0e",
            "b2de3db8401a4b5783dd6c875d10464b",
            "cb9c3f5fcdc944a0beecc2ec45e9a122",
            "94cfa57259a449e5806d3133d3442bc4",
            "c051711fbbbe4dfeb23515d89ec6a41f",
            "7df5a6a2f5f14d7e8c340d50fd719825",
            "00b4617723de4b20bee300b3328f3515",
            "42954974cb1a4e48a6ba7e73462d49c5",
            "77e836f4047d491aa344114ec29340db",
            "bef02b41380e41bc81c6801520c9fb55",
            "ba415cc3c47d40cbb60ba371c21ec2bb",
            "40f0ebd6791045e9a843db5f577dc60f",
            "3945b1af99824ef3904e095d259768a1",
            "74bbb129890f4e1c9b79fc23a6eeb480",
            "16e13a02ac9b4bb2bad24721a4adfcdd",
            "b1cac0c108d64e888536794b5eedfaec",
            "4906878581f94edaab157dccc5f14ce2",
            "7a3b3da8bd62418692fd56d86524655e",
            "700684d6bd3a42dda3dbbe3a3184ffbe",
            "4127d9edc8ba4cb182fc775b47e74125",
            "fb04008488174c07a35ec11f941b7cdb",
            "2b332e5ef7994b4abb19c098eebcf577",
            "bcee3b82a55840f48618cf721979aba8",
            "d67ab9cea2dc44579276efbbdd843190",
            "d763f6a3d9c8438180ec3373f8a91996",
            "f498acb03d6a4cb2ba3bb18a45e920cd",
            "ed6e5c9be66e4b77828f71f1e4c0f8dd",
            "36575f730daa47569b0c8f17ee2e4edb",
            "d0eb81c77c994a6cae4f5d21e0b6c594",
            "422ea9e756fb4c76bef40a8d7f12856e",
            "835e8d3b9ea649d0967f183e89888349",
            "1dcd68e71d444a0d99daecc3e4f37fe8",
            "0b159b68c42b4eedaf71c6142e3a7014",
            "054f8c687c804552a8e36d099020e20c",
            "0d3278dc2b3647a5bd1caa1ba67c415a",
            "7178692a59234105a06d27bc5fa2ed69",
            "87bfec3fe6d44a1989926182ae91689e",
            "758bee854a5b4184a9d7bf38ebc0e580",
            "cd3ee9860c8942c29c56c5aa48043c25",
            "978521f7ff394fbdae2b6bf79071d382",
            "0f2d549895b34f8787c0af1142c95714",
            "7dd11ac6ed5d4292ae605b7af95e6d7d",
            "d8c8867372044f1f95f701e1a5fd21d3",
            "5a7e8bbc60de4e619a1e0fa8888da17b",
            "beb4693285104b67827789b8a7f43207",
            "dbe83ffc375241a7ba65418c2bfeda6c",
            "0cb1c726096049498f7a0bc015fdbe48",
            "4e8d4c19d1734de6a8e9141497c669f8",
            "8aa5424f8fde4c62aecb2eb6cf1a54f8",
            "a33407163c8245d6b6750f7f2c597e4c",
            "28af3ea3c7724ce2962d30b9f5265106",
            "7f46907d07f740d4b18b6b261acc3937",
            "0a42b643e78646c4977affe2260eee32",
            "60eff463678f4948a23ebcdfaf412963",
            "32086e6bc0204dd2a6c3b607f2074490",
            "8ac6462663bc445e8e440d255c393d16",
            "ae8b47cebed6443e86071e7bd963d248",
            "da7be676318442d592bacc00ec35e790",
            "3d3f32f82dac45ac86dcb142f0fac61d",
            "e7ccbaa57c5c44a698d0574d36923837",
            "ed446c74f5a2403082ce7faad2e1ee33",
            "50bdc6257eb644aaa42406353398251d",
            "d3e56c8dfe3d491b86eb8b821c457cb7",
            "8317f658e45f4b6e861da6f934928767",
            "e951aea7b36d492b9099482501284961",
            "ab17420b744840aaa643af4c2a400723",
            "ab5dfaed495a407fa80d814b90d90bb8",
            "92ec9f66680a4d8fab65be063c39e413",
            "0d41c5340e9e4d1fbd0a70587809d01f",
            "cbf7c76de05f4a4e942ba835f135cc40",
            "2b294edc01044489b05c6af2746fb804",
            "42d2904184bd4bf59e31ab5661560816",
            "2104a5315b6c4825833b0401b5107de0",
            "90d48263d5c540be8db9c3550304ad7c",
            "6ba63e6adbbf41489118b816146908b2",
            "90b150b685c34722a58b91d603656347",
            "107f089f4a06426f9e33059083cbd72c",
            "c96a95dde2f14dc8a35e22ab6ca80e5d",
            "98c9418007c347008af8dede777ae4d5",
            "ce365ca8acc44be9a793666f0d678c47",
            "57cd473c7f2c4d33915d7bc913bbf0f5",
            "23fcaaae605d4cd5a47b1f236ce67989",
            "edd3723039c54c088f62689272017844",
            "2bce244d0bf44c3e89656e017ad21390",
            "4dd2925ed83b4ca2909f602510d39b75",
            "91a3b12eb14c48e7a6f8c45a2727ea3d",
            "d45e8e0e588c45ecaf28f814b3ed5fd6",
            "aa7e038ec848466698751ffdc57f9a1b",
            "da10e521a579448891808faf86e8e943",
            "547388870c9e48779cc6a18c37b4c6fa",
            "a5bfba19e63a46828f63e49a5888b790",
            "075cbe3e382b4a0ba20d161b2b28d785",
            "d02140d1701d4894943334b5c4ead0a6",
            "2bbcf5d0eaab490b84d2b9a72ddf9ed1",
            "d872cc4c5171418982bbe36fe8c389d2",
            "e1a61ef5f77844e998701953501e7e5a"
          ]
        },
        "id": "mxv5oBdEQ45y",
        "outputId": "36f1be80-1679-4d44-9744-120c0a589fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b82c1ac52f64d31a60a33ef85c34994"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6582a57b7724a2d8ed49882e604d2b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)model.bin.index.json:   0%|          | 0.00/41.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8ff2afd81fe4207bf62eadd256dca0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a60a8a2d20a4b6fb0f3e791d55dfaa8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77e836f4047d491aa344114ec29340db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4127d9edc8ba4cb182fc775b47e74125"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "835e8d3b9ea649d0967f183e89888349"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dd11ac6ed5d4292ae605b7af95e6d7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a42b643e78646c4977affe2260eee32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8317f658e45f4b6e861da6f934928767"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ba63e6adbbf41489118b816146908b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91a3b12eb14c48e7a6f8c45a2727ea3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'More and more large language models are opensourced so Hugging Face has decided to open source their own'}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}